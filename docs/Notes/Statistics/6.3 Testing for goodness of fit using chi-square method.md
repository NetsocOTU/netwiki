The method described in this section is commonly used in two circumstances:
- Given a sample of cases that can be classified into several groups, determine if the sample is representative of the general population
- Evaluate whether data resemble a particular distribution, such as a normal distribution or a geometric distribution
---
# Example
Are jurors racially representative of the overall population?
![](Acrobat_RLyHJDvAUC.png)
![](Acrobat_lEbABMz8gg.png)
$H_0$: The jurors are a random sample, i.e. there is no racial bias, and the observed counts reflect natural sampling fluctuation.
$H_A$: The jurors are not randomly sampled, i.e. there is racial bias in juror selection. 

In previous hypothesis tests, we constructed a test statistic of the following form: (point estimate - null value) / SE of point estimate. In this scenario, we must construct a test statistic for each group:
- White
	- $Z_1 = \frac{205-198}{\sqrt{198}} = 0.50$
- Black
	- $Z_2=\frac{26-19.25}{\sqrt{19.25}}=1.54$
- Hispanic
	- $Z_3=\frac{25-33}{\sqrt{33}}=-1.39$
- Other
	- $Z_4=\frac{19-24.75}{\sqrt{24.75}}=-1.16$
We take the squared value of each of these Z's and sum them. Using the squared value has two effects:
- Any negative values are now positive
- Unusual differences will become much larger and easier to notice
$Z^2_1 + Z^2_2 + Z^2_3 + Z^2_4 = 5.89 = X^2$
![](Acrobat_PUGG9Ucd5y.png)
$X^2$ summarizes how strongly the observed counts tend to deviate from the null counts. If the null hypothesis is true, then $X^2$ follows a chi-square distribution.

---

The **chi-square distribution** is sometimes used to characterize data sets and statistics that are always positive and typically right skewed. 

While a normal distribution has two parameters - mean and standard deviation - chi-square only has one, called **degrees of freedom (df)**.
![](Acrobat_R5fIs48Jem.png)
As **df** gets larger, 
- the center becomes larger
- the variability increases
- the distribution is more symmetric

# Finding a p-value for a chi-square distribution
A large $X^2$ value suggests strong evidence favouring the alternative hypothesis (that there was racial bias). However, we cannot quantify what the chance was of observing such a large test statistic if the null hypothesis actually was true. This is where the chi-square distribution comes in. If the null hypothesis is true and there is no racial bias, $X^2$ would follow a chi-square distribution with three degrees of freedom. Under certain conditions, $X^2$ follows a chi-square distribution with $k-1$ degrees of freedom, where $k$ is the number of bins. 

---
# Definitions
## Chi-square test for one-way table
Suppose we are to evaluate whether there is convincing evidence that a set of observed counts $O_1, O_2,...O_k$ in $k$ categories are unusually different from what might be expected under a null hypothesis. Call the *expected counts* that are based on the null hypothesis $E_1,E_2,...E_k$. If each expected count is at least 5 and the null hypothesis is true, then the test statistic below follows a chi-square distribution with $k-1$ degrees of freedom:
$$X^2=\frac{(O_1-E_1)^2}{E_1}+\frac{(O_2-E_2)^2}{E_2}+...+\frac{(O_k-E_k)^2}{E_k}+$$
The p-value for this test statistic is found by looking at the upper tail of this chi-square distribution. We consider the upper tail because large values of $X^2$ would provide greater evidence against the null hypothesis. 

## Conditions for Chi-square test
There are two conditions that must be checked before performing a chi-square test:
- Independence - Each case must be independent of all others in the table
- Sample size/distribution - each scenario must have at least 5 expected cases
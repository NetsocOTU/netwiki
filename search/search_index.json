{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This repository is intended to serve as a resource hub for the Networking and IT Security program at Ontario Tech University, providing study resources and general information related to each course.</p> <p>Please note that some information may eventually become outdated. For instance, a course's textbook may change, or a course that originally didn't have an exam may have one added. If you notice such a discrepancy, you can either make your own commit and open a Pull Request, or open an issue to have one of the maintainers fix it.</p>"},{"location":"#academic-integrity-policy","title":"Academic Integrity Policy","text":"<p>All students are welcome to contribute. However, please note that you may not add any resources that violate the school's academic integrity guidelines - for instance, lecture slides, projects/assignments, quizzes/exams, or copyrighted textbooks. If you attempt to add material to the repository that violates this policy, you will be banned from contributing and potentially reported to the school. If you are unsure whether something violates the guidelines, please contact a maintainer directly.</p>"},{"location":"Career%20Resources/Certifications/","title":"Certifications","text":"<p>Certifications are a great way to show your competence to employers. During your time in the program, there will be a few opportunities to get discounts on certification vouchers, or course credit for certifications. </p>"},{"location":"Career%20Resources/Certifications/#cisco-certifications","title":"Cisco Certifications","text":"<p>Ontario Tech is partnered with Cisco as a Cisco Academy. While enrolled in any of the networking classes, your instructors will give you access to NetAcad courses. If you complete the courses and pass the practice exams with a 75% or higher average, you are able to claim a discount voucher for the relevant certification.</p> <p>The first year networking classes follow CCNA content. Year two networking (plus third year's troubleshooting class) follow CCNP content.</p>"},{"location":"Career%20Resources/Certifications/#amazon","title":"Amazon","text":"<p>In the third year INFR 2670U - Introduction to Cloud Services course, you will have the opportunity to get course credit for obtaining a cloud certification. While the certification you choose can be for any of the major cloud providers (AWS, Google Cloud Platform, Microsoft Azure), most people choose AWS due to it having the largest market share. </p> <p>Amazon offers a program in which you can claim a 100% off voucher, allowing you to get your certification for free. The catch is: the process of obtaining this voucher will take months, and you need to start well before you start the Cloud Services class in order to obtain it in time. </p> <p>Note that this information applies to the AWS Certified Cloud Practitioner certification. The process for other certifications may be slightly different. </p> <p>First, you need to complete the two following AWS Skill Builder courses - Exam Prep Official Question Set: AWS Certified Cloud Practitioner (CLF-C02 - English) - Exam Prep Standard Course: AWS Certified Cloud Practitioner (CLF-C02 - English)</p> <p>After you have completed these courses, you need to join the AWS Emerging Talent Community. Once signed up, you will have access to activities that you can complete for a small amount of points. You need 3500 points to redeem the voucher, but there are only so many activities you can complete each week, which will typically net you 20-100 points. </p>"},{"location":"Career%20Resources/Certifications/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Career%20Resources/Certifications/#security","title":"Security","text":""},{"location":"Career%20Resources/Certifications/#-blue-team-level-1","title":"- Blue Team Level 1","text":""},{"location":"Career%20Resources/Resumes/","title":"Resumes","text":"<ul> <li>15 Free ATS Resume Templates<ul> <li>Applicant Tracking Systems (ATS) are used by organizations to basically scan applicant resumes. It's a way that organizations automate the process of filtering out bad resumes. However, ATSes prefer certain resume layouts/formats, so it's important that you use an ATS compatible template so you don't slip through the cracks.</li> </ul> </li> <li>RxResume<ul> <li>Free, open-source, self-hostable resume builder</li> </ul> </li> </ul>"},{"location":"Classes/Program%20Map/","title":"Program Map","text":"<p>Source</p>"},{"location":"Classes/Program%20Map/#year-1","title":"Year 1","text":""},{"location":"Classes/Program%20Map/#fall","title":"Fall","text":"<ul> <li>INFR 1411U - Introduction to Networking I</li> <li>INFR 1010U - Discrete Mathematics</li> <li>INFR 1101U - Introduction to Programming for IT</li> <li>BUSI 1030U - Business Communications</li> <li>BUSI 1600U - Management of the Enterprise</li> </ul>"},{"location":"Classes/Program%20Map/#winter","title":"Winter","text":"<ul> <li>INFR 1421U - Introduction to Networking II</li> <li>INFR 1016U - Introductory Calculus</li> <li>INFR 2141U - Object Oriented Programming for IT</li> <li>INFR 2810U - Computer Architecture</li> <li>BUSI 2000U - Collaborative Leadership</li> </ul>"},{"location":"Classes/Program%20Map/#year-2","title":"Year 2","text":""},{"location":"Classes/Program%20Map/#fall_1","title":"Fall","text":"<ul> <li>INFR 2411U - Advanced Networking I</li> <li>INFR 3120U - Web and Script Programming</li> <li>INFR 2600U - Introduction to Computer Security</li> <li>INFR 1400U - Statistics and Probability for IT</li> <li>INFR 1550U - Law and Ethics of IT</li> </ul>"},{"location":"Classes/Program%20Map/#winter_1","title":"Winter","text":"<ul> <li>INFR 2421U - Advanced Networking II</li> <li>INFR 2820U - Algorithms and Data Structures</li> <li>INFR 2830U - Operating Systems</li> <li>General Elective</li> <li>General Elective</li> </ul>"},{"location":"Classes/Program%20Map/#year-3","title":"Year 3","text":""},{"location":"Classes/Program%20Map/#fall_2","title":"Fall","text":"<ul> <li>INFR 2431U - Advanced Networking III</li> <li>INFR 2670U - Introduction to Cloud Services</li> <li>INFR 3600U - Cryptography and Network Security</li> <li>INFR 3700U - Machine Learning</li> <li>BUSI 2550U - Introduction to Project Management</li> </ul>"},{"location":"Classes/Program%20Map/#winter_2","title":"Winter","text":"<ul> <li>INFR 3610U - Operating System Security</li> <li>INFR 3720U - Basics of Digital Transmission</li> <li>INFR 3810U - Database Systems</li> <li>INFR 3850U - System and Network Administration</li> <li>General Elective</li> </ul>"},{"location":"Classes/Program%20Map/#year-4","title":"Year 4","text":""},{"location":"Classes/Program%20Map/#fall_3","title":"Fall","text":"<ul> <li>INFR 4680U - IT Security Policies and Procedures</li> <li>INFR 4661U - Introduction to Penetration Testing</li> <li>Open Elective</li> <li>Open Elective</li> <li>Technical Elective</li> </ul>"},{"location":"Classes/Program%20Map/#winter_3","title":"Winter","text":"<ul> <li>INFR 4690U - IT Forensics</li> <li>XBIT 4500U - Capstone or XBIT 4600U - Internship</li> <li>Open Elective</li> <li>Technical Elective</li> <li>Technical Elective</li> </ul>"},{"location":"Classes/Electives/INFR%204641U%20-%20Advanced%20Network%20Security/","title":"INFR 4641U   Advanced Network Security","text":"<ul> <li>Textbook: Network Security, Firewalls, and VPNs, Third Edition</li> <li>Lab/Tutorial: Lab</li> <li>Exam: Midterm + final</li> <li>Prerequisites: </li> </ul>"},{"location":"Classes/Electives/INFR%204641U%20-%20Advanced%20Network%20Security/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Fall/BUSI%201030U%20-%20Business%20Communications/","title":"BUSI 1030U   Business Communications","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: </li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year1/Fall/BUSI%201030U%20-%20Business%20Communications/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Fall/BUSI%201600U%20-%20Management%20of%20the%20Enterprise/","title":"BUSI 1600U   Management of the Enterprise","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: </li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year1/Fall/BUSI%201600U%20-%20Management%20of%20the%20Enterprise/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Fall/INFR%201010U%20-%20Discrete%20Mathematics/","title":"INFR 1010U   Discrete Mathematics","text":"<ul> <li>Textbook: Discrete Mathematics and Its Applications (7th or 8th edition will work)</li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year1/Fall/INFR%201010U%20-%20Discrete%20Mathematics/#study-resources","title":"Study Resources","text":"<ul> <li>Discrete Math I (Entire Course)<ul> <li>YouTube playlist by a university professor, based on 7th edition of the textbook</li> </ul> </li> <li>Big-O Explained</li> </ul>"},{"location":"Classes/Year1/Fall/INFR%201101U%20-%20Introduction%20to%20Programming%20for%20IT/","title":"INFR 1101U   Introduction to Programming for IT","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year1/Fall/INFR%201101U%20-%20Introduction%20to%20Programming%20for%20IT/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Fall/INFR%201411U%20-%20Introduction%20to%20Networking%20I/","title":"INFR 1411U   Introduction to Networking I","text":"<ul> <li>Textbook: CCNA 200-301 Official Cert Guide (vol. 1)</li> <li>Lab/Tutorial: Lab</li> <li>Exam: Theory + practical</li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year1/Fall/INFR%201411U%20-%20Introduction%20to%20Networking%20I/#study-resources","title":"Study Resources","text":"<ul> <li>Jeremy's IT Lab<ul> <li>Full free CCNA course with videos, flash cards, labs</li> <li>YouTube </li> <li>Google Drive with resources (flashcards and labs)</li> </ul> </li> <li>Practical Networking - YouTube<ul> <li>Lots of great educational videos. Particularly useful is the Subnetting Mastery playlist</li> </ul> </li> <li>subnetting.org<ul> <li>Randomly generated subnetting practice problems</li> </ul> </li> <li>Subnetting cheatsheet</li> <li>Wendell Odom - CertSkills<ul> <li>CCNA-level labs from the author of the CCNA Original Cisco Guide</li> </ul> </li> <li>Keith Barker - Packet Tracer Labs<ul> <li>26-video playlist of CCNA-level Packet Tracer labs</li> </ul> </li> </ul>"},{"location":"Classes/Year1/Winter/BUSI%202000U%20-%20Collaborative%20Leadership/","title":"BUSI 2000U   Collaborative Leadership","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: </li> <li>Prerequisites:</li> </ul>"},{"location":"Classes/Year1/Winter/BUSI%202000U%20-%20Collaborative%20Leadership/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Winter/INFR%201016U%20-%20Introductory%20Calculus/","title":"INFR 1016U   Introductory Calculus","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites:</li> </ul>"},{"location":"Classes/Year1/Winter/INFR%201016U%20-%20Introductory%20Calculus/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Winter/INFR%201421U%20-%20Introduction%20to%20Networking%20II/","title":"INFR 1421U   Introduction to Networking II","text":"<ul> <li>Textbook: CCNA 200-301 Official Cert Guide (vol. 2)</li> <li>Lab/Tutorial: Lab</li> <li>Exam: Theory + practical</li> <li>Prerequisites: INFR 1411U - Introduction to Networking I</li> </ul>"},{"location":"Classes/Year1/Winter/INFR%201421U%20-%20Introduction%20to%20Networking%20II/#study-resources","title":"Study Resources","text":"<ul> <li>Jeremy's IT Lab<ul> <li>Full free CCNA course with videos, flash cards, labs</li> <li>YouTube </li> <li>Google Drive with resources (flashcards and labs)</li> </ul> </li> <li>Practical Networking - YouTube<ul> <li>Lots of great educational videos. Particularly useful is the Subnetting Mastery playlist</li> </ul> </li> <li>subnetting.org<ul> <li>Randomly generated subnetting practice problems</li> </ul> </li> <li>Subnetting cheatsheet</li> <li>Wendell Odom - CertSkills<ul> <li>CCNA-level labs from the author of the CCNA Original Cisco Guide</li> </ul> </li> <li>Keith Barker - Packet Tracer Labs<ul> <li>26-video playlist of CCNA-level Packet Tracer labs</li> </ul> </li> </ul>"},{"location":"Classes/Year1/Winter/INFR%202141U%20-%20Object%20Oriented%20Programming%20for%20IT/","title":"INFR 2141U   Object Oriented Programming for IT","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1101U - Introduction to Programming for IT</li> </ul>"},{"location":"Classes/Year1/Winter/INFR%202141U%20-%20Object%20Oriented%20Programming%20for%20IT/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year1/Winter/INFR%202810U%20-%20Computer%20Architecture/","title":"INFR 2810U   Computer Architecture","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1101U - Introduction to Programming for IT</li> </ul>"},{"location":"Classes/Year1/Winter/INFR%202810U%20-%20Computer%20Architecture/#study-resources","title":"Study Resources","text":"<ul> <li>Nandgame<ul> <li>Browser game that takes you through the process of building a CPU. Will help you understand basic logic gates and how CPUs work, but goes much more in depth than is actually required for the course.</li> </ul> </li> <li>MIPS Assembly Programming Simplified<ul> <li>38-video YouTube playlist that teaches you everything you need to know about MIPS assembly, which is sometimes used in the course depending on who the instructor is.</li> </ul> </li> <li>EXAPUNKS<ul> <li>Game that uses an assembly-style language to have the player solve puzzles. Very challenging but fun and can help you understand the assembly programming experience.</li> </ul> </li> </ul>"},{"location":"Classes/Year2/Fall/INFR%201400U%20-%20Statistics%20and%20Probability%20for%20IT/","title":"INFR 1400U   Statistics and Probability for IT","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1010U - Discrete Mathematics, INFR 1016U - Introductory Calculus</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%201400U%20-%20Statistics%20and%20Probability%20for%20IT/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Fall/INFR%201550U%20-%20Law%20and%20Ethics%20of%20IT/","title":"INFR 1550U   Law and Ethics of IT","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: Theory</li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%201550U%20-%20Law%20and%20Ethics%20of%20IT/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/","title":"INFR 2411U   Advanced Networking I","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Both</li> <li>Exam: Theory + practical</li> <li>Prerequisites: INFR 1421U - Introduction to Networking II</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/#study-resources","title":"Study Resources","text":"<ul> <li>Enterprise Lab (YouTube playlist)</li> <li>CCIE Enterprise Infrastructure Foundation<ul> <li>Full of challenging labs</li> </ul> </li> <li>PCAP Analysis</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/#ip-routing-essentials","title":"IP Routing Essentials","text":"<ul> <li>Subnetting cheat sheet</li> <li>How does NAT conserve IPv4 addresses?</li> <li>Subnetting Mastery</li> <li>IPv6 IP Addresses Explained </li> <li>Route Precedence - How do routers choose a path when multiple paths exist?</li> <li>EGP vs IGP, Distance Vector vs Link State - Dynamic Routing Protocols</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/#eigrp","title":"EIGRP","text":"<ul> <li>EIGRP Load Balancing</li> <li>EIGRP Deep Dive</li> <li>EIGRP Explained</li> <li>EIGRP DUAL</li> <li>EIGRP Queries and Stuck in Active</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/#ospf","title":"OSPF","text":"<ul> <li>Comparing OSPF with EIGRP</li> <li>Practical OSPF [23 videos]</li> <li>OSPF Passive Interface</li> <li>OSPF Deep Dive</li> <li>OSPF Explained</li> <li>OSPF Multi Area Explained</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202411U%20-%20Advanced%20Networking%20I/#bgp","title":"BGP","text":"<ul> <li>BGP Summarization</li> <li>BGP Weight Tutorial</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202600U%20-%20Introduction%20to%20Computer%20Security/","title":"INFR 2600U   Introduction to Computer Security","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Lab</li> <li>Exam: Theory</li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%202600U%20-%20Introduction%20to%20Computer%20Security/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Fall/INFR%203120U%20-%20Web%20and%20Script%20Programming/","title":"INFR 3120U   Web and Script Programming","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 2141U - Object Oriented Programming for IT</li> </ul>"},{"location":"Classes/Year2/Fall/INFR%203120U%20-%20Web%20and%20Script%20Programming/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/","title":"INFR 2421U   Advanced Networking II","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Both</li> <li>Exam: Theory + practical</li> <li>Prerequisites: INFR 2411U - Advanced Networking I</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#network-architecture-and-switching-fundamentals","title":"Network Architecture and Switching Fundamentals","text":"<ul> <li>Cisco Express Forwarding</li> <li>Connecting Cisco StackWise on the 9300</li> <li> Access, Distribution, and Core Layers Explained</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#spanning-tree","title":"Spanning Tree","text":"<ul> <li>STP Deep Dive</li> <li>STP Explained</li> <li>Introduction to Spanning-Tree</li> <li>STP</li> <li>STP Explained</li> <li>Rapid STP</li> <li>RSTP Explained - With Follow-Along Lab</li> <li>RSTP</li> <li>RSTP Proposal/Agreement Process</li> <li>Master STP Topologies in 3 Steps</li> <li>Multiple Spanning Tree</li> <li>Multiple Spanning Tree</li> <li>MST Explained &amp; Configuration</li> <li>Configure MST on Cisco Switches</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#vlans-ip-services-and-security","title":"VLANs, IP Services, and Security","text":"<ul> <li>GLBP</li> <li>GLBP Operation</li> <li>Understanding and Configuring GLBP</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#vpns","title":"VPNs","text":"<ul> <li>What is MPLS?</li> <li>Phase 2 and 3 DMVPN</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202421U%20-%20Advanced%20Networking%20II/#qos","title":"QoS","text":"<ul> <li>TCP Slow Start &amp; Global Synchronization</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202820U%20-%20Algorithms%20and%20Data%20Structures/","title":"INFR 2820U   Algorithms and Data Structures","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1010U - Discrete Mathematics, INFR 1016U - Introductory Calculus, INFR 2141U - Object Oriented Programming for IT</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202820U%20-%20Algorithms%20and%20Data%20Structures/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year2/Winter/INFR%202830U%20-%20Operating%20Systems/","title":"INFR 2830U   Operating Systems","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1101U - Introduction to Programming for IT</li> </ul>"},{"location":"Classes/Year2/Winter/INFR%202830U%20-%20Operating%20Systems/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Fall/BUSI%202550U%20-%20Introduction%20to%20Project%20Management/","title":"BUSI 2550U   Introduction to Project Management","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: </li> <li>Prerequisites: N/A</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%202431U%20-%20Advanced%20Networking%20III/","title":"INFR 2431U   Advanced Networking III","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Lab</li> <li>Exam: </li> <li>Prerequisites: INFR 2421U - Advanced Networking II</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%202431U%20-%20Advanced%20Networking%20III/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Fall/INFR%202670U%20-%20Introduction%20to%20Cloud%20Services/","title":"INFR 2670U   Introduction to Cloud Services","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: N/A</li> <li>Prerequisites: INFR 2810U - Computer Architecture</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%202670U%20-%20Introduction%20to%20Cloud%20Services/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Fall/INFR%203600U%20-%20Cryptography%20and%20Network%20Security/","title":"INFR 3600U   Cryptography and Network Security","text":"<ul> <li>Textbook: Cryptography and Network Security</li> <li>Lab/Tutorial: </li> <li>Exam: </li> <li>Prerequisites: INFR 1010U - Discrete Mathematics, INFR 2600U - Introduction to Computer Security</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%203600U%20-%20Cryptography%20and%20Network%20Security/#study-resources","title":"Study Resources","text":"<ul> <li>The Code Book by Simon Singh; entertaining educational book about the history of cryptography, covering everything from the early ancient methods to modern algorithms like PGP. </li> <li>Cryptohack.org - \"A free, fun platform for learning modern cryptography.\" Similar to something like TryHackMe but focused entirely on cryptography and programming</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%203700U%20-%20Machine%20Learning/","title":"INFR 3700U   Machine Learning","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: </li> <li>Exam: </li> <li>Prerequisites: INFR 1101U - Introduction to Programming for IT, INFR 2141U - Object Oriented Programming for IT, INFR 1400U - Statistics and Probability for IT</li> </ul>"},{"location":"Classes/Year3/Fall/INFR%203700U%20-%20Machine%20Learning/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Winter/INFR%203610U%20-%20Operating%20System%20Security/","title":"INFR 3610U   Operating System Security","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: N/A</li> <li>Prerequisites: INFR 2600U - Introduction to Computer Security</li> </ul>"},{"location":"Classes/Year3/Winter/INFR%203610U%20-%20Operating%20System%20Security/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Winter/INFR%203720U%20-%20Basics%20of%20Digital%20Transmission/","title":"INFR 3720U   Basics of Digital Transmission","text":"<ul> <li>Textbook: Data Communications and Networking</li> <li>Lab/Tutorial: Tutorial</li> <li>Exam: Theory</li> <li>Prerequisites: INFR 1400U - Statistics and Probability for IT, INFR 1421U - Introduction to Networking II</li> </ul>"},{"location":"Classes/Year3/Winter/INFR%203720U%20-%20Basics%20of%20Digital%20Transmission/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Winter/INFR%203810U%20-%20Database%20Systems/","title":"INFR 3810U   Database Systems","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: N/A</li> <li>Prerequisites: INFR 2820U - Algorithms and Data Structures</li> </ul>"},{"location":"Classes/Year3/Winter/INFR%203810U%20-%20Database%20Systems/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year3/Winter/INFR%203850U%20-%20System%20and%20Network%20Administration/","title":"INFR 3850U   System and Network Administration","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Lab</li> <li>Exam: Theory, practical</li> <li>Prerequisites: INFR 2421U - Advanced Networking II, INFR 2830U - Operating Systems</li> </ul>"},{"location":"Classes/Year3/Winter/INFR%203850U%20-%20System%20and%20Network%20Administration/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year4/Fall/INFR%204661U%20-%20Introduction%20to%20Penetration%20Testing/","title":"INFR 4661U   Introduction to Penetration Testing","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: Lab</li> <li>Exam: </li> <li>Prerequisites: INFR 2820U - Algorithms and Data Structures, INFR 3120U - Web and Script Programming, INFR 3810U - Database Systems</li> </ul>"},{"location":"Classes/Year4/Fall/INFR%204661U%20-%20Introduction%20to%20Penetration%20Testing/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year4/Fall/INFR%204680U%20-%20IT%20Security%20Policies%20and%20Procedures/","title":"INFR 4680U   IT Security Policies and Procedures","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: N/A</li> <li>Exam: </li> <li>Prerequisites: INFR 1550U - Law and Ethics of IT, INFR 3600U - Cryptography and Network Security, INFR 1010U - Discrete Mathematics</li> </ul>"},{"location":"Classes/Year4/Fall/INFR%204680U%20-%20IT%20Security%20Policies%20and%20Procedures/#study-resources","title":"Study Resources","text":""},{"location":"Classes/Year4/Winter/INFR%204690U%20-%20IT%20Forensics/","title":"INFR 4690U   IT Forensics","text":"<ul> <li>Textbook: </li> <li>Lab/Tutorial: </li> <li>Exam: </li> <li>Prerequisites: INFR 2421U - Advanced Networking II, INFR 3600U - Cryptography and Network Security</li> </ul>"},{"location":"Classes/Year4/Winter/INFR%204690U%20-%20IT%20Forensics/#study-resources","title":"Study Resources","text":""},{"location":"Classes/YearX/Technical%20Elective/","title":"Technical Elective","text":""},{"location":"Classes/YearX/XBIT%204500U%20-%20Capstone/","title":"XBIT 4500U   Capstone","text":"<ul> <li>Textbook: N/A</li> <li>Lab/Tutorial: N/A</li> <li>Exam: N/A</li> <li>Prerequisites:</li> </ul>"},{"location":"Classes/YearX/XBIT%204600U%20-%20Internship/","title":"XBIT 4600U   Internship","text":"<ul> <li>Textbook: N/A</li> <li>Lab/Tutorial: N/A</li> <li>Exam: N/A</li> <li>Prerequisites:</li> </ul> <p>The internship course is one of the most important courses in the program. In order to graduate, you must complete either the internship course or the Capstone course. Most people choose to do an internship, because it provides the opportunity to get some real-world work experience, and to get paid for it. Internships are typically taken during 3rd or 4th year. </p> <p>To get started planning for your internship, head over to the Student Life Portal. Once you're signed in, head to the Co-Op &amp; Internship section.</p> <p></p> <p>You must first submit an application to determine your eligibility for the internship program. After your application is approved, you will be able to view job postings and apply to them directly through the Student Life Portal. Companies posting here are specifically looking for students from OTU, so this is the best place to find your first work opportunity. </p>"},{"location":"Classes/YearX/XBIT%204600U%20-%20Internship/#eligibility","title":"Eligibility","text":"<p>To be eligible for the internship program, you must: - Have completed 54 credit hours (3rd year standing) - Have at least a 2.3 cumulative GPA</p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/","title":"Brief History of Malware","text":"<p>Source</p> <p>Malware is a portmanteau for malicious software. Malware is typically designed to cause damage to a computer or network. </p> <p>The first concepts for malicious software originated in 1949. One of the first concepts was created by John von Neumann, who published a paper describing a self-replicating computer program. The concept was not proven until 1971.</p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#creeper-1974","title":"Creeper - 1974","text":"<p>The Creeper Program was the first virus ever created. It was written by Bob Thomas and used ARPANET to transfer itself to other computers and erase itself afterwards. It was written in PDP-10 Assembly for the operating system TENEX. \"computers fail from time to time and work is lost. So I got interested in the possibility of moving an executing program from one computer to another without interrupting the ongoing operation of the program, at least to the extent that to an external observer nothing had happened.\" Fortunately, John wasn't malicious - the virus simply displayed a message that said \"I'm the creeper, catch me if you can!\"</p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#reaper","title":"Reaper","text":"<p>Ray Tomlinson, who also recreated a version of Creeper which did not delete itself, and invented e-mail, created Reaper not long after Creeper was released.  Reaper's purpose was, funnily enough, to delete Creeper if it found it on the infected machine.</p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#wabbit-1974","title":"Wabbit - 1974","text":"<p>Wabbit was written in 1974 and was one of the first examples of self-replicating malware. It was named Wabbit because it worked very quickly; so quickly, in fact, that the system would normally run out of processing power and end up crashing. It is known as the first truly malicious program and grew from concepts created by other computer scientists. It was not a worm (it did not travel over a network), it only infected the machine it was on. </p> <p>Today, it would be referred to as a type of Denial of Service (DoS) attack known as a fork bomb. </p> <p>It works by creating an infinite loop that creates system processes and copies of the original file. This caused an abnormally high number of CPU cycles and would clog the system, causing it to crash - this is the concept of a fork bomb. </p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#animal-1975","title":"ANIMAL - 1975","text":"<p>In 1975, the first Trojan virus was written, called ANIMAL, created by John Walker. It would act as a game which asked the user a number of questions in order to guess the animal they were thinking of.  While the user was playing the \"game\", a program in the background was running. PERVADE would create a copy of itself and ANIMAL in every directory that the user had access to. The term Trojan refers to the Trojan Horse of Ancient Greece. The Greeks built a giant wooden horse and sent it as a gift to the independent city of Troy - but had hidden armed men inside it, who broke out and infiltrated the city. This is analogous to how the virus hides itself, making the user think it is an innocent program while the malicious part happens in the background.  However, ANIMAL was not made with malicious intent. It was written to make sure that the directory structure was not damaged. It was eventually stopped by an OS upgrade, which changed the file status tables that PERVADE used to copy the files. </p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#elk-cloner-1982","title":"Elk Cloner - 1982","text":"<p>In 1982, a 15 year old high school student named Richard Skrenta created one of the first microcomputer viruses that spread outside of a controlled system (i.e. a single computer or laboratory).  Elk Cloner attached itself to the Apple II operating system and spread through floppy disks. Skrenta wrote it as a prank on his friends.  The technique it used is now called boot sector virus. The program was placed in a game and did nothing until the 50th time the game was opened, upon which it would change to a blank screen and display a poem about the virus.  If the computer booted from an infected floppy disk, the virus was placed in the computers memory; meaning it would then spread to any disk inserted into the computer.</p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#cascade-1988","title":"Cascade - 1988","text":"<p>Cascade was the first type of malware to use encryption - it did not directly damage user data, but simply kept the program undetected.  It was meant to specifcally infect Digital Equipment computers, including code that attempted to identify if the program was running on an IBM computer, and if it was, then exit the program - however, this failed and IBM headquarters in Belgium were infected, prompting them to release a private antivirus software. </p>"},{"location":"Miscellaneous/Brief%20History%20of%20Malware/#morris-internet-worm","title":"Morris Internet Worm","text":"<p>In 1988, Robert Tappan Morris created a worm that was supposed to point out security flaws in the academic networks it infected. It worked, but he failed to implement a measure to check if the computer it was on had already been infected, thus causing crashes on many computers through a DoS attack.  Morris was the first person in the United States to be arrested under the 1986 Comptuer Fraud and Abuse Act.  The worm spread by exploiting publicly known vulnerabilities in Unix Sendmail, remote shells, and weak passwords, and did lead to global awareness of the danger of having a weak password.  It infected 2000 computers within 15 hours and often took more than 2 days to remove from a single computer. It reportedly infected a total of 6000 computers, around 10% of the Internet at the time. </p>"},{"location":"Miscellaneous/Domain%20Tools/","title":"Domain Tools","text":"<p>Info</p> <p>Tools related to domains and/or IP addresses, ASNs, certificates, etc.</p>"},{"location":"Miscellaneous/Domain%20Tools/#domains","title":"Domains","text":""},{"location":"Miscellaneous/Domain%20Tools/#web-tools","title":"Web Tools","text":"<ul> <li>URLVoid - check domain reputation</li> <li>DNSDumpster - query DNS records, geoIP, subdomains, domain map</li> <li>crt.sh - query TLS certificates for a domain</li> </ul>"},{"location":"Miscellaneous/Domain%20Tools/#cli","title":"CLI","text":"<ul> <li>Amass<ul> <li><code>amass enum -src -ip -brute -d [url]</code></li> <li>Slowest, but most thorough</li> </ul> </li> <li>Sublist3r<ul> <li><code>python3 sublist3r.py -d [url]</code></li> <li>Fast, but only finds common subdomains</li> </ul> </li> <li>Photon</li> </ul>"},{"location":"Miscellaneous/Domain%20Tools/#ip-addresses","title":"IP Addresses","text":"<ul> <li>IPVoid</li> <li>AbuseIPDB</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/","title":"Educational Platforms","text":"<p>Info</p> <p>Hands-on security education platforms. May not be directly relevant to a particular class.</p>"},{"location":"Miscellaneous/Educational%20Platforms/#general","title":"General","text":"<ul> <li>TryHackMe - CTFs and educational rooms with machines to hack into.</li> <li>HackTheBox - Similar to TryHackMe but more advanced.</li> <li>LetsDefend - Hands on SOC analyst training</li> <li>Defend the Web - Interactive training on various security topics</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#ctfs","title":"CTFs","text":"<ul> <li>Hacker101</li> <li>CTFd Reverse Engineering educational CTF</li> <li>PicoCTF</li> <li>CTFtime - Find upcoming CTFs</li> <li>Microcorruption - Embedded devices CTF</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#challenges","title":"Challenges","text":"<ul> <li>Codewars - Coding challenges for various languages</li> <li>OSINT Dojo - OSINT challenges</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#war-games","title":"War Games","text":"<ul> <li>OverTheWire </li> <li>The Ethernaut </li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#exploits","title":"Exploits","text":"<ul> <li>XSS Games</li> <li>Pwnable.kr</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#cryptography","title":"Cryptography","text":"<ul> <li>CryptoHack </li> <li>CryptoPals </li> <li>MysteryTwister </li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#reverse-engineering","title":"Reverse Engineering","text":"<ul> <li>Crackmes </li> <li>Reversing.kr</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#labs","title":"Labs","text":"<ul> <li>PentesterLab</li> <li>Authlab</li> </ul>"},{"location":"Miscellaneous/Educational%20Platforms/#vulnerable-apps","title":"Vulnerable Apps","text":"<ul> <li>bWAPP - An intentionally vulnerable webapp (100+ vulnerabilities)</li> <li>Gruyere </li> <li>OWASP Juice Shop</li> <li>Generic University</li> <li>CVWA</li> <li>Hackazon</li> <li>OWASP Mutillidae</li> <li>OWASP Hackademic</li> <li>Damn Vulnerable Web Application</li> <li>Vulnhub</li> </ul>"},{"location":"Miscellaneous/Hacker%20Methodology/","title":"Hacker Methodology","text":"<p>Source</p>"},{"location":"Miscellaneous/Hacker%20Methodology/#reconnaissance","title":"Reconnaissance","text":"<p>Reconnaissance is the process of gathering information about a target. Generally, it does not involve interaction with the target. </p> <p>Reconnaissance might involve Googling a company to learn about its history or tools it uses, reading its Wikipedia, or the company's social media. </p> <p>Examples of recon tools:</p> <ul> <li>Google (specifically Google Dorking)</li> <li>Wikipedia</li> <li>PeopleFinder.com</li> <li>who.is</li> <li>sublist3r</li> <li>hunter.io</li> <li>builtwith.com</li> <li>wappalyzer</li> </ul>"},{"location":"Miscellaneous/Hacker%20Methodology/#enumeration-and-scanning","title":"Enumeration and Scanning","text":"<p>This phase involves directly interacting with the target to gather information about it. It involves more specialized tools and produces more useful results. The goal of this phase is to define and narrow the attack surface. </p> <p>Examples of enumeration/scanning tools: - Nmap - Gobuster - Metasploit - exploit-db - Burp Suite</p>"},{"location":"Miscellaneous/Hacker%20Methodology/#exploitation","title":"Exploitation","text":"<p>This phase is when the knowledge gained in recon and enumeration is actually used to access the machine.</p> <p>Examples of exploit tools: - Metasploit - Burp Suite - SQLMap - BeEF</p>"},{"location":"Miscellaneous/Hacker%20Methodology/#privilege-escalation","title":"Privilege Escalation","text":"<p>After access to the machine is gained, privileges must be exploited so the hacker can execute their goal, which generally requires root privileges, to access parts of the system and commands that they should not be able to.</p> <p>Examples of privilege escalation: -   Cracking password hashes found on the target -   Finding a vulnerable service or version of a service which will allow you to escalate privilege THROUGH the service -   Password spraying of previously discovered credentials (password re-use) -   Using default credentials -   Finding secret keys or SSH keys stored on a device which will allow pivoting to another machine -   Running scripts or commands to enumerate system settings like <code>ifconfig</code> to find network settings, or the command <code>find / -perm \\-4000 -type f 2&gt;/dev/null</code> to see if the user has access to any commands they can run as root</p>"},{"location":"Miscellaneous/Hacker%20Methodology/#covering-tracks","title":"Covering Tracks","text":"<p>While not required in most professional hacking scenarios, this is still a part of the general methodology. In a professional scenario, the penetration tester should stop immediately when they have escalated privileges, therefore the hacker won't need to cover their tracks because the test was planned and consensual. Often, however, the pen-tester will need to assist the IT administrator in fixing the vulnerability, so it is important to keep detailed notes of your actions. </p>"},{"location":"Miscellaneous/Hacker%20Methodology/#reporting","title":"Reporting","text":"<p>This phase involves outlining everything you have found in a report. It often includes the vulnerabilities, the criticality of the finding, a description of how it was found, and recommendations for patching it. </p> <p>Example report by Heath Adams</p>"},{"location":"Miscellaneous/Hashes%20and%20Passwords/","title":"Hashes and Passwords","text":"<ul> <li>Rawsec's CyberSecurity Inventory<ul> <li>Large list of tools/wordlists</li> </ul> </li> </ul>"},{"location":"Miscellaneous/Hashes%20and%20Passwords/#cracking","title":"Cracking","text":"<ul> <li>Haiti - identify hashes, tell you the hashcat/john mode to use<ul> <li>Install with <code>gem install haiti-hash</code></li> </ul> </li> <li>Search That Hash - identify hashes and attempt to crack them</li> <li>Hashcat<ul> <li>Hash mode examples</li> </ul> </li> <li>John the Ripper<ul> <li><code>john &lt;hashfile&gt; --format=&lt;format&gt; --wordlist=&lt;/path/to/wordlist&gt;</code></li> <li>Formats</li> </ul> </li> </ul>"},{"location":"Miscellaneous/Hashes%20and%20Passwords/#online-tools","title":"Online Tools","text":"<ul> <li>Hashes.com</li> <li>CrackStation</li> </ul>"},{"location":"Miscellaneous/Hashes%20and%20Passwords/#wordlists","title":"Wordlists","text":"<ul> <li>SecLists </li> <li>wordlistctl </li> </ul>"},{"location":"Miscellaneous/Hashes%20and%20Passwords/#generating-wordlists","title":"Generating Wordlists","text":"<ul> <li>Mentalist - given a list, creates variations of it (i.e. hello -&gt; h3ll0)  </li> <li>CeWL - grab words from a URL \u00a0 \u00a0 - e.g. download all words from example.org with a depth of 2: <code>cewl -d 2 -w $(pwd)/example.txt https://example.org</code> </li> <li>TTPassGen \u00a0 \u00a0 - e.g. create wordlist containing all combinations of 4 digits <code>ttpassgen --rule '[?d]{4:4:*}' out.txt</code> \u00a0 \u00a0 - all lowercase character combinations of length 1 - 3 <code>ttpassgen --rule '[?l]{1:3:*}' out.txt</code> \u00a0 \u00a0 - combination of the above: <code>ttpassgen --dictlist 'pin.txt,abc.txt' --rule '$0[-]{1}$1' combination.txt</code></li> </ul>"},{"location":"Miscellaneous/OSINT/","title":"OSINT","text":"<ul> <li>IntelTechniques</li> <li>OH SHINT!</li> <li>OSINT4ALL</li> <li>OSINT Framework</li> <li>Comprehensive list of OSINT tools part 1, 2, 3</li> <li>OSINT VM from TraceLabs: https://www.tracelabs.org/initiatives/osint-vm</li> </ul>"},{"location":"Miscellaneous/OSINT/#domains-ips-certificates-etc","title":"Domains, IPs, Certificates, etc.","text":"<p>See Domain Tools</p>"},{"location":"Miscellaneous/OSINT/#maps","title":"Maps","text":"<ul> <li>Google Earth Pro</li> <li>OpenInfraMap</li> <li>ADS-B Exchange - flight tracking</li> </ul>"},{"location":"Miscellaneous/OSINT/#email","title":"Email","text":"<ul> <li>theHarvester - Find e-mails, subdomains, and names</li> <li>Hunter.io - Find e-mails for a certain domain</li> <li>Holehe - test if an email is registered on various services</li> </ul>"},{"location":"Miscellaneous/OSINT/#facial-recognition","title":"Facial Recognition","text":"<ul> <li>PimEyes - Input an image of a face, receive likely matches</li> </ul>"},{"location":"Miscellaneous/OSINT/#search-engines","title":"Search Engines","text":"<ul> <li>You.com - GPT3 powered</li> </ul>"},{"location":"Miscellaneous/OSINT/#internet","title":"Internet","text":"<ul> <li>ASN Lookup Tools</li> <li>Shodan - and alternatives</li> </ul>"},{"location":"Miscellaneous/OSINT/#phone-numbers","title":"Phone Numbers","text":"<ul> <li>PhoneInfoga</li> </ul>"},{"location":"Miscellaneous/OSINT/#usernames","title":"Usernames","text":"<p>Scan various sites to see if a given username exists:</p> <ul> <li>Sherlock</li> <li>WhatsMyName - checks more sites but takes longer</li> </ul>"},{"location":"Miscellaneous/OSINT/#misc","title":"Misc","text":"<ul> <li>CarNET.ai - identify car make/model</li> <li>EyeWitness - take screenshots of web pages</li> </ul>"},{"location":"Miscellaneous/OT%20Resources/","title":"OT Resources","text":""},{"location":"Miscellaneous/OT%20Resources/#repositories","title":"Repositories","text":"<ul> <li>Awesome Industrial Control System Security</li> <li>Industrial Control System Security Tools</li> <li>Awesome Security Resources</li> </ul>"},{"location":"Miscellaneous/OT%20Resources/#textbooks","title":"Textbooks","text":"<ul> <li>Practical Industrial Cybersecurity: ICS, Industry 4.0, and IIoT</li> <li>NIST: Guide to OT Security</li> <li>Industrial Network Security: Securing Critical Infrastructure Networks for Smart Grid, SCADA, and Other Industrial Control Systems</li> <li>Pentesting Industrial Control Systems: An ethical hacker's guide to analyzing, compromising, mitigating, and securing industrial processes</li> <li>A Dragos Industrial Control System Security Reading List<ul> <li>Very thorough reading list following Dragos' training course outline. Dozens of books/papers included.</li> </ul> </li> </ul>"},{"location":"Miscellaneous/OT%20Resources/#misc-reading","title":"Misc. Reading","text":"<ul> <li>SANS Institute: Practical Industrial Control System (ICS) Cybersecurity: IT and OT Have Converged - Discover and Defend Your Assets<ul> <li>Overview of how the convergence of IT and OT is enabling better management of control systems, but also introducing problems, particularly with security.</li> </ul> </li> <li>SANS Institute: ICS Defense: It's Not a \"copy-paste\" from an IT playbook<ul> <li>Description of how OT security requires a different approach than traditional IT security.</li> </ul> </li> <li>The Threat to ICS<ul> <li>SANS talk discussing the threats to ICS and the gaps in current security.</li> </ul> </li> <li>SANS ICS Security YouTube Channel<ul> <li>223 videos, mostly talks, about ICS security</li> </ul> </li> <li>SANS Institute ICS Library<ul> <li>Collection of various resources similar to above.</li> </ul> </li> <li>Dragos Inc.: Analyzing the Threat to Electric Grid Operations</li> <li>Dragos Inc.: The Industrial Cyber Threat Landscape: The role of the private sector and government in addressing cyber threats to energy infrastructure<ul> <li>Transcript of a hearing before the Committee on Energy and Natural Resources of the US Senate.\u00a0 Mostly focuses on regulation, but description of the threat landscape is good.</li> </ul> </li> </ul>"},{"location":"Miscellaneous/OT%20Resources/#professional-training","title":"Professional Training","text":"<ul> <li>ICS cybersecurity academy - https://ics-cybersecurity.academy/next-trainings/online-training/</li> <li>CISA ICS training (free, web-based) - https://www.cisa.gov/ics-training-available-through-cisa</li> <li>SANS ICS410 - https://www.sans.org/cyber-security-courses/ics-scada-cyber-security-essentials/ - meant to provide a bridge for people with prior experience in one field but not the other (ICS or security)</li> <li>SANS ICS612 - https://www.sans.org/cyber-security-courses/ics-cyber-security-in-depth/ - entry level overview of ICS security</li> <li>IAEA Nuclear Security E-Learning - https://www.iaea.org/topics/security-of-nuclear-and-other-radioactive-material/nuclear-security-e-learning - free, online learning modules</li> <li>ICS Cyber Security Institute training - https://icscsi.org/curriculum-assessingics.html</li> </ul>"},{"location":"Miscellaneous/OT%20Resources/#labs","title":"Labs","text":""},{"location":"Miscellaneous/OT%20Resources/#free-virtual","title":"Free &amp; Virtual","text":"<ul> <li>ICS Security labs with VM exports, slides, notes etc.</li> <li>Simulated ICS network teaching about buffer overflow attacks; VM exports available</li> <li>OT Security labs</li> </ul>"},{"location":"Miscellaneous/OT%20Resources/#paid-andor-physical","title":"Paid and/or Physical","text":"<ul> <li>EC-Council ICS/SCADA Cyber Security lab environment</li> <li>Idaho National Laboratory lab environment</li> <li>PwC OT Cyber Lab</li> <li>SCADA Testbed (paper providing description of testbed)</li> <li>Norway's Directorate of Water Resources and Energy description of lab environment</li> </ul>"},{"location":"Miscellaneous/Red%20Team/","title":"Red Team","text":""},{"location":"Miscellaneous/Red%20Team/#resources","title":"Resources","text":"<ul> <li>PEASS-ng</li> <li>LOLBAS</li> <li>Red Team Notes</li> <li>HackTricks</li> <li>GTFObins</li> </ul>"},{"location":"Miscellaneous/Red%20Team/#blog-posts","title":"Blog Posts","text":"<ul> <li>Combining Direct System Calls and sRDI to bypass AV/EDR</li> </ul>"},{"location":"Miscellaneous/Research%20Resources/","title":"Research Resources","text":"<p>This is a list of useful links to multiple Ontario Tech research resources to assist with academic and/or technical writing and research. The first few are worth bookmarking!</p> <ul> <li>Networking &amp; IT Security - Research Articles Databases</li> <li>Ontario Tech Citation &amp; Writing Reference</li> <li>Ontario Tech Library OMNI Search</li> <li>Research Database Lookup by Subject</li> <li>(External) KPU Guide to Doing Research</li> <li>Book a Research Appointment with an OTU Subject Librarian</li> <li>Ontario Tech NOOL - Academic Writing Resources</li> <li>MyBib Citation &amp; Bibliography Generator (double check your entries, it can make mistakes)</li> </ul>"},{"location":"Miscellaneous/Resources/","title":"Resources","text":""},{"location":"Miscellaneous/Resources/#resource-lists","title":"Resource Lists","text":"<ul> <li>PayloadAllTheThings</li> <li>Awesome Security</li> <li>Awesome CTF</li> <li>Awesome Web Security</li> <li>Awesome Hacking</li> <li>Awesome Ethical Hacking Resources</li> <li>Awesome Infosec</li> <li>Awesome Hacking Tools</li> <li>Awesome Hacking (different from above)</li> <li>Awesome Hacking Resources</li> <li>Awesome Web Hacking</li> <li>Penetration Testing</li> <li>Awesome Pentest</li> <li>Awesome Pentest (different from above)</li> </ul>"},{"location":"Miscellaneous/Resources/#blogs","title":"Blogs","text":"<ul> <li>PenTestMonkey</li> <li>SkullSecurity</li> <li>Null Byte</li> <li>FuzzySecurity - tutorials, scripts, exploits, and a list of resources</li> </ul>"},{"location":"Miscellaneous/Resources/#interesting-posts","title":"Interesting Posts","text":"<ul> <li>MD5 Collision Demo</li> <li>Hardware and IoT Hacking Resources</li> </ul>"},{"location":"Miscellaneous/Resources/#ctf-writeups","title":"CTF Writeups","text":"<ul> <li>ctf-writeups</li> <li>best-web-ctf-writeups</li> </ul>"},{"location":"Miscellaneous/Resources/#courses","title":"Courses","text":"<ul> <li>freeCodeCamp - Python for PenTesting</li> <li>[Collection of university courses](https://github.com/prakhar1989/awesome-courses#security</li> <li>Pentester Academy</li> <li>Invoke-RE</li> <li>IoT and Hardware Hacking</li> </ul>"},{"location":"Miscellaneous/Resources/#projects","title":"Projects","text":"<ul> <li>Build your own network stack</li> <li>Blueteam Homelab</li> <li>How to establish a honeypot on your network</li> <li>Creating a honeypot</li> <li>SIEM Home Lab with ELK</li> <li>100 Redteam Projects</li> </ul>"},{"location":"Miscellaneous/Resources/#books","title":"Books","text":"<ul> <li>Reverse Engineering for Beginners [PDF]</li> <li>Security Engineering</li> <li>Cryptoparty Handbook [HTML]</li> <li>An Introduction to Computer Networks [HTML, PDF, epub]</li> <li>Computer Networking: Principles, Protocols, and Practice [HTML]</li> <li>Computer Networks: A Systems Approach [HTML]</li> <li>The Crypto Dictionary[PDF, epub]</li> </ul>"},{"location":"Miscellaneous/Resources/#subreddits","title":"Subreddits","text":"<p>Note: not all security related, but general interest IT-related subreddits that I find helpful.</p> <ul> <li>AskNetSec</li> <li>Cybersecurity</li> <li>CCNA</li> <li>Homelab</li> <li>Selfhosted</li> <li>Sysadmin</li> <li>OSINT</li> <li>OpSec</li> <li>Networking</li> <li>NetSecStudents</li> <li>NetSec</li> <li>MSP</li> <li>ITCareerQuestions</li> </ul>"},{"location":"Miscellaneous/Resources/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Miscellaneous/Resources/#web-application-tools","title":"Web Application Tools","text":"<ul> <li>Burp Suite</li> <li>OWASP ZAP<ul> <li>Completely free alternative to Burp</li> </ul> </li> </ul>"},{"location":"Miscellaneous/Resources/#steganography","title":"Steganography","text":"<ul> <li>AperiSolve<ul> <li>Runs all of the following on an image: <code>zsteg, steghide, outguess, exiftool, binwalk, foremost, strings</code></li> </ul> </li> <li>Stegoveritas</li> <li>Sonic Visualizer</li> </ul>"},{"location":"Miscellaneous/Resources/#port-scanning","title":"Port Scanning","text":"<ul> <li>Nmap</li> <li>RustScan<ul> <li>Very fast</li> </ul> </li> </ul>"},{"location":"Miscellaneous/Resources/#networking","title":"Networking","text":"<ul> <li>Impacket <pre><code>git clone https://github.com/fortra/impacket.git\ncd impacket\npip3 install -r requirements.txt\nsudo python3 setup.py install\n</code></pre></li> </ul>"},{"location":"Miscellaneous/Resources/#cheat-sheets","title":"Cheat Sheets","text":"<ul> <li>Reverse Shell Cheatsheet</li> <li>Snort (scroll down to Conclusion)</li> <li></li> <li></li> <li></li> <li></li> </ul>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/","title":"Steganography Crash Course","text":"<p>Source</p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#steghide","title":"Steghide","text":"<p>Steghide is used to hide information in JPGs (note that it only works on JPGs). Install with <code>apt install steghide</code>. Flags - <code>embed</code> or <code>--embed</code>: embeds secret data in a cover file - <code>-ef</code> or <code>--embedfile</code>: specify file to embed data to - <code>-cf</code> or <code>--coverfile</code>: specify cover file (must be one of the following formats: AU, BMP, JPEG, WAV)     - <code>-p</code>: specify passphrase to use for the cover file - <code>-sf</code> or <code>--stegofile</code>: specify the name for the stego file that will be created from the operation - <code>extract</code>: extract data from a file     - <code>-sf</code> or <code>--stegofile</code>: specify name of file to extract from</p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#zsteg","title":"Zsteg","text":"<p>Equivalent to Steghide, but for PNG and BMP. Install with <code>gem install zsteg</code>. - <code>--lsb</code>: Least significant bit comes first - <code>--msb</code>: Most significant bit comes first - <code>-E &lt;name&gt;</code>: extract specified payload</p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#exiftool","title":"Exiftool","text":"<p>Technically not a steganography tool, but useful for viewing metadata of images. <code>apt install exiftool</code></p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#stegoveritas","title":"Stegoveritas","text":"<p>Supports most image file formats. Contains many built in tests to extract data, which is useful if you're not sure what you're looking for. <code>pip3 install stegoveritas</code>, <code>stegoveritas_install_deps</code> - <code>-meta</code>: check file for metadata info - <code>-steghide</code>: check steghide for hidden info - <code>-extractLSB</code>: extract a specific LSB RGB from the image; used with <code>-red</code>, <code>-green</code>, <code>-blue</code>, <code>-alpha</code></p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#spectrograms","title":"Spectrograms","text":"<p>Use SonicVisualizer. Import file, go Layer &gt; Add Spectrogram</p>"},{"location":"Miscellaneous/Steganography%20Crash%20Course/#qr-codes","title":"QR Codes","text":"<p>Use zbar - <code>sudo apt install zbar-tools</code> <code>zbarimg &lt;file&gt;</code></p>"},{"location":"Notes/Big-O%20Explained/","title":"Big O Explained","text":"<p>Originally posted on February 6, 2023 by Hayden Nolan </p> <p>Big-O (along with big-Theta and big-Omega) are concepts in mathematics and computer science that were popularized by the legendary Donald Knuth. If you're studying anything to do with computers, you'll likely run into this concept at some point, and it can be hard to wrap your head around at first. \u00a0  </p> <p>TL;DR, \\(f(x)=O(g(x))\\) with witnesses \\(C, k\\) means that \\(f(x) \\le Cg(x) \\forall x \\ge k\\) </p> <p> </p> <p>Given a mathematical function, \\(f(x)\\), the big-O of this function gives us an upper bound for how \\(f(x)\\) scales as \\(x\\) gets bigger. In the graph above, when \\(x\\) is less than 1, \\(f(x)\\) is greater than \\(4x^2\\), but when \\(x\\) is greater than 1, \\(f(x)\\) will always be smaller than \\(4x^2\\).  </p> <p>We use O here because the growth rate of a function can also be called the order of the function. You may also have seen this called the degree of the function. To find the order of a function, we essentially just add the exponents of each term, and the highest exponent is that function's degree/order.  </p> <p>\\(f(x) = 4x^2\\) - this function only has one term (\\(4x^2\\)), and only one exponent, 2, so the order/degree is 2.  </p> <p>\\(g(x)= 4x^2 + 2x^3\\) - this function has two terms (\\(4x^2\\) and \\(2x^3\\)). Since 3 is a larger exponent than 2, the order/degree is 3.  </p> <p>\\(h(x) = 4x^2 y^3 + 2x^3 y\\) - this is where the addition of the exponents comes in. The first term has exponents 2 and 3, so we add them and find that the degree is 5.  </p> <p>Going back to the image example: There are two important values to note, called witnesses.  </p> <p>The first witness is the x-axis value of 1, and it is usually represented by \\(k\\). It's important because, from looking at the graph, we can see that when \\(x&lt;1\\), \\(f(x)\\) is actually larger than \\(4x^2\\). \\(k = 1\\) says \"\\(4x^2\\) will always be larger than \\(f(x)\\) when \\(x &gt; 1\\).  </p> <p>The second witness is the constant 4 (from \\(4x^2\\)). Look at the third function in the image, which is \\(x^2\\). Notice how it is always less than \\(f(x)\\), and this makes sense because both have a degree of 2. If we have \\(x^2\\) and \\(x^2 + 5\\), the second one will obviously always be larger. However, if we have \\(x^2 + 5\\) and \\(2x^2\\), we can see that after a certain value of \\(x\\), the constant multiple takes over and \\(2x^2\\) is larger from there on. See Desmos example.  </p> <p>Tying everything all together, we have the following:  </p> <p>\\(O(g(x))\\) and \\(C\\) - \\(g(x)\\) is the function which is an upper bound for \\(f(x)\\) when it has a constant multiple of \\(C\\). \\(g(x)\\) on its own might not be an upper bound for \\(f(x)\\), but \\(C \\cdot g(x)\\) is.  </p> <p>\\(k\\) - the x-value where \\(g(x)\\) actually is an upper bound. For x-values smaller than \\(k\\), \\(f(x)\\) might be larger than \\(g(x)\\), but once we reach \\(f(k)\\), \\(C \\cdot g(x)\\) will always be larger than \\(f(x)\\).  </p> <p>For me, big-O and complexity really clicked when I started thinking about them in the context of algorithms or programming functions. If you have two functions which do the same thing in a different way, which one is faster? Is it always faster, or only when the input is small? Why is one faster than the other?  </p> <p>Also, note that there can be infinitely many pairs of witnesses. In the Desmos example that I linked, \\(C = 2\\) and \\(k = 2.236\\), but we could've changed C to any number - for example, if \\(C= 5\\), and then k moves to 1.118. There are no two values that are \"the\" witnesses, we just need to find any two values that work.  </p> <p>Once you understand the concept of big-O, the related concepts of big-Theta and big-Omega are easy. Big-O tells us the upper bound, big-Omega tells us the lower bound, and big-Theta tells us both.</p>"},{"location":"Notes/Introduction%20to%20APIs/","title":"Introduction to APIs","text":"<p>Source</p>"},{"location":"Notes/Introduction%20to%20APIs/#apis","title":"APIs","text":"<ul> <li>Application Programming Interface<ul> <li>allows apps to talk to each other</li> </ul> </li> <li>Synchronous API<ul> <li>respond to request directly, provide data immediately</li> <li>used when data is readily available, i.e. stored in database/memory - server can fetch data instantly</li> </ul> </li> <li>Asynchronous API<ul> <li>Provide response to let user know request has been received, but response doesn't contain data - server processes request (may take a while) and sends notif (or triggers callback) when data is ready</li> <li>Used when request is an action that takes time for server to process or if data not readily available</li> </ul> </li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#common-architectural-styles","title":"Common Architectural Styles","text":"<ul> <li>Remote Procedure Call (RPC)<ul> <li>Request/response model</li> <li>Lets app (client) make procedure call to another app (server)</li> <li>Client typically unaware that the procedure request is being executed remotely; request made to a layer that hides those details</li> <li></li> <li>Commonly, client makes synchronous req to the server and is blocked while server processes request; when server is done, sends response back which unblocks the process</li> <li>Example implementations<ul> <li>XML-RPC, JSON-RPC, NFS, SOAP</li> </ul> </li> </ul> </li> <li>Simple Object Access Protocol (SOAP)<ul> <li>messaging protocol for communicating between apps that might be on different platforms/built with different langs</li> <li>XML-based (considered an application of XML), developed by Microsoft</li> <li>Commonly used with HTTP but can be applied to other protocols - SMTP, TCP, UDP, JMS</li> <li>Messages contain four elements<ul> <li>Envelope, Header, Body, Fault </li> <li>Envelope: root element, tells you the XML doc is a SOAP message</li> <li>Header: optional; if present, must be first child of Envelope. Contains app-specific info like authorization, attributes</li> <li>Body: contains data to be sent to server; must be XML and in its own namespace</li> <li>Fault: optional, must be child of Body if present. Provides error/status info. Can only be one fault element in a SOAP message</li> </ul> </li> </ul> </li> <li>Representational State Transfer (REST)<ul> <li>Architectural design; can be applied to any protocol</li> <li>Six constraints<ul> <li>Client-server: should be independent of each other</li> <li>Stateless: requests from client must contain all info server needs to make the request, server can't maintain session states</li> <li>Cache: responses from server must indicate whether response is cacheable - if it is, client can use the data for later requests</li> <li>Uniform interface: must adhere to these 4 principles<ul> <li>Identification of resources - resource must be identified in the request as the object the server will access and manipulate</li> <li>Manipulation of resources through representations - client receives representation from the server, which must contain enough data/metadata for client to be able to manipulate the resource</li> <li>Self-descriptive messages - each message must contain all info for recipient to process the message</li> <li>Hypermedia as the engine of application state - data sent by server must include additional actions/resources available for the client to access supplemental info about the resource</li> </ul> </li> <li>Layered system: hierarchical layers, each layer provides service only to the layer above it</li> <li>Code-on-demand: optional; info returned by a REST service can include executable code</li> </ul> </li> </ul> </li> <li>URI/URL<ul> <li><code>scheme:[//authority][/path][?query]</code></li> <li></li> </ul> </li> <li>Headers<ul> <li>Request headers - include additional info not related to content of the message, like Authorization</li> <li>Entity headers - additional info that describe the content of the body of the message, i.e. Content-Type</li> <li>Response headers - contain additional info not related to content of message, e.g. Set-Cookie, Cache-Control</li> </ul> </li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#sequence-diagrams","title":"Sequence Diagrams","text":"<ul> <li>aka Event Diagrams</li> <li>Interactions with REST APIs are typically a sequence of requests, not a single one</li> <li>Unified Modeling Language (UML)</li> <li>Y-axis is time, X-axis is \"lifelines\" - exchanges/messages, any element that can interact by receiving/sending a message </li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#authentication","title":"Authentication","text":""},{"location":"Notes/Introduction%20to%20APIs/#mechanisms","title":"Mechanisms","text":"<ul> <li>Basic Auth<ul> <li>User/pass transmitted as header: <code>Authorization: Basic &lt;user&gt;:&lt;pass&gt;</code></li> </ul> </li> <li>Bearer Auth/Token Auth<ul> <li><code>Authorization: Bearer &lt;token&gt;</code></li> <li>More secure; used with OAuth, SSO</li> </ul> </li> <li>API key/token<ul> <li><code>Authorization: &lt;api key&gt;</code> </li> <li><code>Authorization: APIkey &lt;api key&gt;</code></li> <li><code>Cookie: API_KEY=&lt;api key&gt;</code></li> <li><code>Content-Type: application/json</code> (token transmitted in body)</li> </ul> </li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#authorization","title":"Authorization","text":"<ul> <li>OAuth combines authentication + authorization</li> <li>Recommended form of authen/author for REST APIs</li> <li>Modern version is OAuth 2.0, not backwards compatible</li> <li>Lets pre-registered apps get authorization to perform REST API requests on user's behalf, without user needing to share credentials with the app itself</li> <li>Lets user provide creds directly to the authorization server (typically an IdP or IdS) to obtain access token that is shared with application</li> <li>Process of obtaining the token is called a flow; app then uses this token in the REST API as Bearer Auth</li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#rate-limiting-algorithms","title":"Rate Limiting Algorithms","text":"<ul> <li>No one standard way, but some common algorithms</li> <li>Leaky bucket: incoming reqs go into a queue, can come in at any rate but server processes at a fixed rate. If queue is full, request is rejected</li> <li>Token bucket: get x tokens per y time, accumulative. When req is made, server checks the bucket to make sure you have at least one token (and rejects if there are no tokens)</li> <li>Fixed window counter: Similar to token bucket, but uses a counter instead of tokens and is not accumulative. The time window is strict, i.e. starts based on clock, not when you make a request </li> <li>Sliding window counter: Similar to fixed window, but the time window begins when you make a request, not based on the clock</li> </ul>"},{"location":"Notes/Introduction%20to%20APIs/#webhooks","title":"Webhooks","text":"<ul> <li>Webhook = HTTP callback (POST to a specific URL that notifies an application of an event occurrence on some monitored resource)</li> <li>aka reverse APIs - applications subscribe to a webhook server by registering with the provider</li> </ul>"},{"location":"Notes/Statistics/","title":"Index","text":"<p>Textbook notes for OpenIntro Statistics - Fourth Edition - all credit goes to the original authors.</p> <p>NOTE: These notes are not exhaustive; they do not cover every chapter/subchapter and some concepts were skipped. </p>"},{"location":"Notes/Statistics/#1-introduction-to-data","title":"1. Introduction to Data","text":"<ul> <li>1.2 Data basics</li> </ul>"},{"location":"Notes/Statistics/#2-summarizing-data","title":"2. Summarizing Data","text":"<ul> <li>2.1 Examining numerical data</li> </ul>"},{"location":"Notes/Statistics/#3-probability","title":"3. Probability","text":"<ul> <li>3.1 Defining probability</li> <li>3.2 Conditional probability</li> <li>3.3 Sampling from a small population</li> <li>3.4 Random variables</li> <li>3.5 Permutations and Combinations</li> </ul>"},{"location":"Notes/Statistics/#4-distributions-of-random-variables","title":"4. Distributions of Random Variables","text":"<ul> <li>4.1 Normal distribution</li> <li>4.2 Geometric distribution</li> <li>4.3 Binomial distribution</li> <li>4.4 Negative binomial distribution</li> <li>4.5 Poisson distribution</li> </ul>"},{"location":"Notes/Statistics/#5-foundations-for-inference","title":"5. Foundations for Inference","text":"<ul> <li>5.1 Point estimates and sampling variability</li> <li>5.2 Confidence intervals for a proportion</li> <li>5.3 Hypothesis testing for a proportion</li> </ul>"},{"location":"Notes/Statistics/#6-inference-for-categorical-data","title":"6. Inference for Categorical Data","text":"<ul> <li>6.1 Inference for a single proportion</li> <li>6.2 Difference of two proportions</li> <li>6.3 Testing for goodness of fit using chi-square method</li> <li>6.4 Testing for independence in two-way tables</li> </ul>"},{"location":"Notes/Statistics/#7-inference-for-numerical-data","title":"7. Inference for Numerical Data","text":"<ul> <li>7.1 One-sample means with the t-distribution</li> <li>7.2 Paired data</li> <li>7.3 Difference of two means</li> <li>7.4 Power calculations for a difference of means</li> <li>7.5 Comparing many means with ANOVA</li> </ul>"},{"location":"Notes/Statistics/#8-introduction-to-linear-regression","title":"8. Introduction to Linear Regression","text":"<ul> <li>8.1 Fitting a line, residuals, and correlation</li> <li>8.2 Least squares regression</li> </ul>"},{"location":"Notes/Statistics/1.2%20Data%20basics/","title":"1.2 Data basics","text":"<ul> <li>Formal name for a row in a table is a case or observational unit<ul> <li>Columns represent characteristics, aka variables</li> </ul> </li> <li>Data matrix is like a table/excel spreadsheet type thing</li> <li>Types of variables<ul> <li>Numerical<ul> <li>Continuous or Discrete</li> </ul> </li> <li>Categorical<ul> <li>Values are called the variable's levels</li> <li>Ordinal variables have a natural order, i.e. 'education level' - high school below university, undergrad below postgrad, etc.</li> <li>Nominal variables have no natural order - i.e. types of fruit</li> </ul> </li> </ul> </li> <li>When two variables show a connection with each other, they are said to be associated or dependent</li> <li> <p>An explanatory variable has an affect on a response variable - i.e. study time affects grade results.</p> </li> <li> <p>An observational study is when you collect data in a way that does not directly interfere with how the data arises</p> <ul> <li>prospective study identifies individuals and collects information as events unfold</li> <li>retrospective study collects data after events have taken place</li> </ul> </li> <li> <p>Simple random sample - equivalent to using a raffle to select cases, all cases in population have an equal chance of being included</p> </li> <li>Non-response bias is introduced when there is a high rate of non-response - certain groups may not be represented because they choose not to respond to surveys</li> <li> <p>Convenience sample - picking the most easily accessible to be included in the sample</p> </li> <li> <p>Making causal conclusions based on observational data is not recommended - generally it only shows associations that can be used to form hypotheses. </p> </li> <li>A confounding variable is correlated to the explanatory and response variables<ul> <li>i.e. study shows that people who have higher use of sunscreen get skin cancer more often</li> <li>the confounding variable is sun exposure time; people who are outside more are more likely to use sunscreen and get skin cancer</li> </ul> </li> </ul>"},{"location":"Notes/Statistics/1.2%20Data%20basics/#four-sampling-methods","title":"Four sampling methods","text":"<ul> <li>Simple random sampling<ul> <li>raffle; all cases in population are equally likely to be included</li> <li>e.g. randomly choosing NBA players to compare salary</li> </ul> </li> <li>Stratified sampling<ul> <li>population divided into groups called strata, chosen so that similar cases are grouped</li> <li>second sampling method, usually simple random, is employed within each stratum</li> <li>e.g. randomly picking 5 members of each NBA team to compare salary</li> </ul> </li> <li>Cluster sampling<ul> <li>Break population into groups (clusters), then sample a fixed number of clusters and include all observations from each of those clusters in the sample</li> <li>multistage sample is similar, but instead of keeping all observations from each cluster, we collect a random sample within each selected cluster</li> <li>Useful when different clusters vary a lot, but within clusters there isn't much variation</li> </ul> </li> </ul>"},{"location":"Notes/Statistics/1.2%20Data%20basics/#experiments","title":"Experiments","text":"<p>Any study where the researchers assign treatments is an experiment. Randomized experiments generally follow four principles: - Controlling - researchers control differences in groups.      - e.g. some people take pills with a small amount of water, others a large amount. Doctors may ask all patients to drink the same volume of water with the pill instead - Randomization - randomize patients into treatment groups to account for variables that cannot be controlled     - e.g. some patients may be more susceptible to a disease because of their diet; randomization helps even out these differences - Replication - the more cases you observe, the more accurately you can estimate the effect of the explanatory variable on the response. Replicate the experiment until you collect a large enough sample. Additionally, studies can be replicated by others to verify their results - Blocking - sometimes variables other than treatment influence the response. When this happens, group individuals based on this variable into blocks, then randomize cases within each block      - e.g. looking at the effect of a drug on heart attacks, we split the patients into low-risk and high-risk, then randomly assign half the patients from each block to the control group and the other half to the treatment group</p> <ul> <li>When a patient doesn't know whether they are in the treatment group or control group, the study is said to be blind</li> <li>A double-blind study is when the researcher also doesn't know which group is which - if they know, they can introduce bias</li> </ul>"},{"location":"Notes/Statistics/2.1%20Examining%20numerical%20data/","title":"2.1 Examining numerical data","text":"<p>The formal name for a row is a case or observational unit. Columns represent characteristics/variables </p> Case 1 Variable 1 Variable 2 <p></p> <p>Box Plot example</p> <ul> <li>The distance of an observation from its mean is its deviation<ul> <li>e.g. if the mean is 5 and the observation is 3, the deviation is 2</li> </ul> </li> <li>The average of the square of deviations is called the variance and is denoted by \\(s^2\\) or \\(\\sigma^2\\)</li> <li>The standard deviation is the square root of the variance and is denoted by \\(s\\) or \\(\\sigma\\)</li> <li> <p>Usually ~70% of data is within one standard deviation of the mean</p> </li> <li> <p>Robust Statistics</p> <ul> <li>The median and IQR are called robust statistics because outliers have little influence on their values</li> <li>Conversely, the mean and standard deviation are more heavily influenced by outliers</li> </ul> </li> <li>When data is strongly skewed, we sometimes transform them so they are easier to model<ul> <li>Consider a histogram of the population of countries, with an x-axis scaling by 100 million.<ul> <li>Almost all data will be in the first bin, and this doesn\u2019t tell us much</li> <li>We could instead take the \\(\\log\\) of the x-axis, allowing us to see much more information about the data<ul> <li>Other common transformations are the square root (\\(\\sqrt{\\text{original observation}}\\)) or inverse \\(\\frac{1}{\\text{original observation}}\\)</li> </ul> </li> </ul> </li> <li>A transformation is a rescaling of data using a function</li> <li>Common goals of transformation are<ul> <li>Seeing the data structure differently</li> <li>Reducing skew</li> <li>Assisting in modelling</li> <li>Straightening a nonlinear relationship in a scatter plot</li> </ul> </li> </ul> </li> </ul> <p> Intensity maps should be used for geographical data; these are maps that use colour to indicate the value of a variable. They are not very helpful for seeing precise values, but they are very helpful for identifying geographic trends.</p> <p> Contingency table - Useful for summarizing data for two or more categorical variables.</p>"},{"location":"Notes/Statistics/3.1%20Defining%20probability/","title":"3.1 Defining probability","text":"<p>The probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times. ****</p> <p>[!def] Law of Large Numbers As more observations are collected, the proportion \\(p_n\\) of occurrences with a particular outcome converges to the probability \\(p\\) of that income.</p> <p>Two outcomes are called disjoint or mutually exclusive if they cannot both happen. For example, you cannot roll both a 1 and 2 on a die at the same time. We compute the probability that one of these outcomes will occur by adding their separate probabilities:</p> <p>\\(P(1 \\text{ or }2) = P(1) + P(2) = 1/6 + 1/6 = 1/3\\)</p> <p>[!def] Addition Rule of Disjoint Outcomes If \\(A_1\\)and \\(A_2\\)represent two disjoint outcomes, then the probability of one of them occurring is given by \\(P(A_1 \\text{ or } A_2) = P(A_1) + P(A_2)\\).</p> <p>Sets of outcomes are often called events. For a die roll, we might have:</p> <p>A = {1, 2} - B = {4, 6} - C = {5} - D = {2, 3}</p> <p>A and B are disjoint events, since they do not share any outcomes. The Addition Rule applies to both outcomes and events.</p>"},{"location":"Notes/Statistics/3.1%20Defining%20probability/#venn-diagrams","title":"Venn Diagrams","text":"<p> This Venn diagram represents all diamond cards on the left side, and all face cards on the right side.</p> <p>This Venn diagram represents all diamond cards on the left side, and all face cards on the right side.</p> <p>Since these events are not disjoint, the Addition Rule does not work. Instead, we can use the Venn diagram.</p> <ul> <li>Add the probabilities of the two events: \\(P(A) + P(B)=P(\\text{diamond})+P(\\text{face card}) = 13/52 +12/52\\)</li> <li>The three cards that are in both events were counted twice, so we must correct: \\(P(A)+P(B)=13/52+12/52-3/52=22/52 = 11/26\\)</li> </ul> <p>This is known as the general addition rule.</p> <p>[!def] General Addition Rule  If \\(A\\) and \\(B\\) are any two events, disjoint or not, then the probability that at least one of them will occur is \\(P(A \\text{ or } B)=P(A)+P(B)-P(A \\text{ and } B)\\), where \\(P(A \\text{ and }B)\\) is the probability that both events occur.</p>"},{"location":"Notes/Statistics/3.1%20Defining%20probability/#probability-distributions","title":"Probability Distributions","text":"<p>A probability distribution is a table of all disjoint outcomes and their associated probabilities. It must follow these rules:</p> <ol> <li>Outcomes listed must be disjoint</li> <li>Each probability must be between 0 to 1</li> <li>The probabilities must total 1</li> </ol> <p>They can also be represented in bar plots. </p>"},{"location":"Notes/Statistics/3.1%20Defining%20probability/#complement-of-an-event","title":"Complement of an Event","text":"<p>The set of values that can be produced by rolling a traditional die is {1, 2, 3, 4, 5, 6}. This set is called the sample space (\\(S\\)) for rolling a die.</p> <p>Let \\(D\\) be some subset of the sample space. The complement of \\(D\\) is all the outcomes in the sample space that are not \\(D\\), and is denoted by \\(D^c\\).</p>"},{"location":"Notes/Statistics/3.1%20Defining%20probability/#independence","title":"Independence","text":"<p>If two results are not related, i.e. knowing the outcome of one provides no useful information on the outcome of the other, they are independent.</p> <p>Rolling two dice is a basic independent process.</p> <p>[!def] Multiplication Rule for Independent Processes If \\(A\\) and \\(B\\) represent independent events, then the probability that both \\(A\\) and \\(B\\) occur can be calculated as the product of their separate probabilities: \\(P(A \\wedge B) = P(A) \\cdot P(B)\\) If there are \\(k\\) events \\(A_1,...,A_k\\) from \\(k\\) independent processes, the probability they all occur is \\(P(A_1\\wedge...\\wedge A_k)=P(A_1)\\cdot...\\cdot P(A_k)\\)</p>"},{"location":"Notes/Statistics/3.2%20Conditional%20probability/","title":"3.2 Conditional probability","text":"<p>[!def] Marginal and Joint Probabilities If a probability is based on a single variable, it is a marginal probability. The probability of outcomes for two or more variables or processes is called a joint probability. Table proportions can be used to summarize joint probabilities.</p> <p>Table proportions can be used to summarize joint probabilities.  Probability table for a machine learning algorithm predicting whether a photo is about fashion or not. </p> <p>A conditional probability is computed under a certain condition. There are two parts to a conditional probability: the outcome of interest and the condition. It is generally written like P(A|B)=... where A is the outcome of interest, and | is read as \u201cgiven.\u201d</p> <p>[!def] Conditional Probability  The conditional probability of outcome \\(A\\) given condition \\(B\\) is computed as \\(P(A|B)=\\frac{P(A \\wedge B)}{P(B)}\\)</p> <p>[!def] General Multiplication Rule  If A and B represent two outcomes or events, then \\(P(A \\wedge B)=P(A|B) \\cdot P(B)\\)  It is useful to think of A as the outcome of interest and B as the condition.</p> <p>[!example]  Consider the <code>smallpox</code> data set. Suppose we only know that 96.08% of residents were not inoculated, and 85.88% of residents who were not inoculated ended up surviving. How could we compute the probability that a resident was not inoculated and lived? We want to determine \\(P(A\\wedge \\overline{B}\\)), where \\(A\\) represents living, and \\(B\\) represents being inoculated. We are given that \\(P(A|\\overline{B})=0.8588\\) and \\(P(\\overline{B})=0.9608\\). Therefore, \\(P(A \\wedge \\overline{B})=0.8588 \\cdot 0.9608 = 0.8251\\)</p>"},{"location":"Notes/Statistics/3.2%20Conditional%20probability/#sum-of-conditional-probabilities","title":"Sum of Conditional Probabilities","text":"<p>[!def] Sum of conditional probabilities Let \\(A_1,...,A_k\\) represent all the disjoint outcomes for a variable or process. Then if \\(B\\) is an event, possibly for another variable or process, we have: \\(P(A_1|B)+...+P(A_k|B)=1\\) The rule for complements also holds when an event and its complement are conditioned on the same information: \\(P(A|B)=1-P(A^c|B)\\)</p> <p>[!info] Gambler\u2019s fallacy X is playing roulette. The previous 5 rolls have landed on black. He knows that the chance of getting black six times in a row is very small, so he bets on red. This is a logical fallacy because the chance of the 6th roll does not depend on the previous ones.</p>"},{"location":"Notes/Statistics/3.2%20Conditional%20probability/#tree-diagrams","title":"Tree Diagrams","text":"<p>Tree diagrams are useful for organizing outcomes and probabilities around the structure of the data. They are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors. </p>"},{"location":"Notes/Statistics/3.2%20Conditional%20probability/#bayes-theorem","title":"Bayes\u2019 Theorem","text":"<p>Sometimes we are given a conditional probability of the form \\(P(\\text{statement about variable 1}|\\text{statement about variable 2})\\), but we really want to know the inverse - \\(P(\\text{statement about variable 2}|\\text{statement about variable 1})\\)</p> <p>Sometimes tree diagrams can help us find the second probability when given the first, but when it is not possible to draw the scenario in a tree diagram, Bayes\u2019 theorem is useful.</p> <p>Consider the following conditional probability for variable 1 and variable 2:</p> \\[P(\\text{outcome } A_1 \\text{ of variable }1|\\text{ outcome }B \\text{ of variable }2)\\] <p>Bayes\u2019 Theorem states that this conditional probability can be identified as the following fraction:</p> \\[ \\frac{P(B|A_1)\\cdot P(A_1)}{P(B|A_1)\\cdot P(B|A_2) \\cdot P(A_2) + ... + P(B|A_k) \\cdot P(A_k)} \\] <p>To apply Bayes\u2019 Theorem correctly, you need to do two preparatory steps:</p> <ol> <li>Identify the marginal probabilities of each possible outcome of the first variable: \\(P(A_1), P(A_2),...,P(A_k)\\)</li> <li>Identify the probability of the outcome \\(B\\), conditioned on each possible scenario for the first variable: \\(P(B|A_1),P(B|A_2),...P(B|A_k)\\)</li> </ol> <p>[!example]  There are academic events on 35% of nights, sporting events on 20% of nights, and no events on 45% of nights. Joe visits campus every Thursday. The table on the right shows the frequency of events on campus and how often the parking lot fills up for each type of event. If Joe comes to campus and finds the garage full, what is the probability that there is a sporting events?</p> Frequency (%) How often lot fills (%) Academic 35 25 Sporting 20 70 None 45 5 Total 100 100 1.  The probability that there is a sporting event and the garage is full is \\(0.20 \\cdot 0.70 = 0.14\\) 2.  The probability that the garage is full is \\((0.35\\cdot 0.25)+(0.14)+(0.45*0.05)=0.0875+0.14+0.0225=0.25\\) 3.  The ratio of these probabilities is the solution: \\(\\frac{0.14}{0.25}=0.56\\). If the garage is full, there is a 56% chance there is a sporting event."},{"location":"Notes/Statistics/3.3%20Sampling%20from%20a%20small%20population/","title":"3.3 Sampling from a small population","text":"<p>Sampling without replacement (i.e. no case can be sampled twice) means we no longer have independence. Example: Imagine there are 10 students in a class, including you. The probability of you being picked for a question is 1/10. If the teacher asks 3 questions and doesn\u2019t ask the same person twice, what is the probability that you will not be picked?</p> <p>9/10 * 8/9 * 7/8 = 0.7</p> <p>There is a 70% chance you won\u2019t get picked.</p> <p>Interestingly, when sampling a small fraction of the total population (under 10%), observations are nearly independent, even when sampling without replacement.</p>"},{"location":"Notes/Statistics/3.4%20Random%20variables/","title":"3.4 Random variables","text":"<p>A variable or process with a numerical outcome is a random variable. We usually represent it with a capital letter such as X, Y, or Z. For example, the amount of money a single student will spend on their textbooks is a random variable.</p> <p>[!def] Expected Value of a Discrete Random Variable If X takes outcomes \\(x_1,...,x_k\\) with probabilities \\(P(X=x_1),...P(X=X_k)\\), the expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\(E(X)=x_1 \\cdot P(X=x_1)+...+P(X=x_k)\\) \\(=\\sum_{i=1}^{k} x_iP(X=x_i)\\) We can represent \\(E(X)\\) as \\(\\mu\\)</p> <p>The expected value for a random variable represents the average outcome. Expected value \\(=E(X)=\\mu\\)</p> <p>This value corresponds to the center of gravity in physics.</p>"},{"location":"Notes/Statistics/3.4%20Random%20variables/#variability-in-random-variables","title":"Variability in Random Variables","text":"<p>[!def] General Variance Formula  If \\(X\\) takes outcomes \\(x_1,...x_k\\)with probabilities \\(P(X=x_1),...,P(X=x_k)\\) and expected value \\(\\mu = E(X)\\), then the variance of \\(X\\) denoted by \\(Var(X)\\) or \\(\\sigma^2\\) is \\(\\(\\begin{align} \\sigma^2 &amp;= (x_1-\\mu)^2 \\cdot P(X=x_1)+...+(x_k-\\mu)^2 \\cdot P(X=x_k) \\\\ &amp;= \\sum_{j=1}^k (x_j-\\mu)^2P(X=x_j) \\end{align}\\)\\) The standard deviation of X, labelled \\sigma, is the square root of the variance.</p>"},{"location":"Notes/Statistics/3.4%20Random%20variables/#example","title":"Example","text":"<p>The following example uses numbers about the rate of student textbook purchase. 20% of students buy neither of the two textbooks, so they spend $0 (1). 55% of students buy only one textbook, spending $137 (2). Finally, 25% of students buy both, spending $170 (3).</p> <p> -   \\(x_i =\\) dollar amount -   \\(x_i \\times P(X=x_i)\\) = dollar amount * probability</p>"},{"location":"Notes/Statistics/3.4%20Random%20variables/#example-2","title":"Example 2","text":"<p>The bookstore offers a textbook for $159 and a study guide for $41. They know that 25% of students buy the textbook and 60% buy both the textbook and supplement.</p> <ol> <li>What proportion of students don\u2019t buy either book? Assume no students buy the supplement without the textbook</li> </ol> Neither One Both Total 15 25 60 100 <ol> <li>Let Y represent the revenue from a single student. Write out the probability distribution of Y</li> </ol> i 1 2 3 Total \\(x_i\\) $0 $159 $200 - \\(P(X=x_i)\\) 0.15 0.25 0.6 1 <ol> <li>Compute the expected revenue from a single student</li> </ol> i 1 2 3 Total \\(x_i\\) $0 $159 $200 - \\(P(X=x_i)\\) 0.15 0.25 0.6 1 \\(x_i \\times P(X=x_i)\\) 0 39.75 120 159.75 <ol> <li>Find the standard deviation to describe the variability associated with the revenue from a single student</li> </ol> i 1 2 3 Total \\(x_i\\) $0 $159 $200 - \\(P(X=x_i)\\) 0.15 0.25 0.6 1 \\(x_i \\times P(X=x_i)\\) 0 39.75 120 159.75 \\(x_i - E(X)\\) -159.75 -0.75 40.25 \\((x_i - E(X))^2\\) 25,520.0625 0.5625 1620.0625 \\((x_i - E(X))^2 \\times P(X=x_i)\\) 3828.01 0.141 972.0375 4800.1885"},{"location":"Notes/Statistics/3.4%20Random%20variables/#linear-combinations-of-random-variables","title":"Linear Combinations of Random Variables","text":"<p>A linear combination of two random variables is a fancy name for an expression like \\(aX+bY\\). For example, imagine a student is going to buy textbooks for the upcoming semester, and they are also going to try to sell their textbooks from last semester. Assume all new textbooks have the same price, $50, and all old textbooks have the same price, $25. The combination for the total amount of gain or loss in money could be represented as \\(a25-b50\\).</p> <p>[!info]  To compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result: \\(a\\cdot E(X) + b \\cdot E(Y)\\)</p> <p>[!def] Variance of linear combinations of random variables  \\(Var(aX+bY)=a^2\\cdot Var(X)+b^2 \\cdot Var(Y)\\) This equality assumes that the variables are independent. The standard deviation may be found by taking the square root of the variance.</p>"},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/","title":"3.5 Permutations and Combinations","text":"<ul> <li>Sample space: the set \\(S\\) of all possible outcomes for a random experiment (an experiment where the outcome depends on random chance)</li> <li>An event is a set of outcomes</li> <li> <p>If a sample space \\(S\\) has \\(n\\) outcomes with equal probability, then an event \\(E\\) consisting of \\(m\\) outcomes has probability \\(P(E)=\\frac{m}{n}\\)</p> </li> <li> <p>Canonical random experiment</p> <ul> <li>\"A box contains \\(n\\) distinct objects; \\(k\\) objects are randomly selected from the box, one at a time.\"</li> <li>Four variations:<ul> <li>Ordered with and without replacement (permutations)</li> <li>Unordered with and without replacement (combination)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#permutations","title":"Permutations","text":""},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#variation-1","title":"Variation 1","text":"<ul> <li>Ordered, no replacement</li> <li>Box has 3 objects numbered 1, 2, and 3. Pick objects one at a time. How many different outcomes are there?<ul> <li>\\(3! = 3\\cdot 2 \\cdot 1 = 6\\)</li> </ul> </li> <li>Same experiment except the box has 5 objects. You only pick 3. How many outcomes are there?<ul> <li>\\(5 \\cdot 4 \\cdot 3 = 60\\)</li> </ul> </li> <li>The number of permutations of \\(n\\) objects is \\(n \\cdot (n-1) \\cdot \\cdot \\cdot 2 \\cdot 1 = n!\\)</li> <li>The number of \\(k\\)-permutations from a set of \\(n\\) is \\(\\(\\large P_{n,k}=\\frac{n!}{(n-k)!}\\)\\)</li> </ul>"},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#variation-2","title":"Variation 2","text":"<ul> <li>Ordered, with replacement</li> <li>Same amount of choices each time</li> <li>\\(k\\)-permutations of \\(n\\) with replacement is \\(n^k\\)</li> </ul>"},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#combinations","title":"Combinations","text":""},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#variation-3","title":"Variation 3","text":"<ul> <li>Unordered, without replacement</li> <li>Consider a box with objects 1, 2, 3, 4. How many ways are there to form groups of 3, if the order does not matter?<ul> <li>\\(\\large \\frac{4!}{3!}=\\frac{24}{6}=4\\)</li> </ul> </li> <li>The number of distinct \\(k\\)-combinations or subsets of \\(k\\) objects from a set of \\(n\\) objects is $$ \\large C_{n,k}=\\frac{n!}{k!(n-k)!}$$</li> </ul>"},{"location":"Notes/Statistics/3.5%20Permutations%20and%20Combinations/#variation-4","title":"Variation 4","text":"<ul> <li>Unordered, with replacement</li> <li>A doughnut shop has 5 different kinds of donuts. How many unique ways are there to select 12 doughnuts? \\(\\(\\large C_{(n+k-1),k}=C_{(5+12-1),12}=C_{16,12}=1820\\)\\)</li> </ul>"},{"location":"Notes/Statistics/4.1%20Normal%20distribution/","title":"4.1 Normal distribution","text":"<p>If a normal distribution has a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\) then we can write the distribution as \\(N(\\mu, \\sigma)\\).</p> <p>The mean and standard deviation are called the parameters of the normal distribution. A normal distribution with a mean of 0 and standard deviation of 1 is called the standard normal distribution.  The above example uses a standardization technique called the Z-score. The Z-score is defined as the number of standard deviations that a case falls above or below the mean. If the observation is 1 standard deviation above the mean, then it has a Z-score of 1. If it is 1.5 standard deviations below the mean, it has a Z-score of -1.5. It is defined mathematically as \\(Z=(x-\\mu)\\div\\sigma\\).</p> <p>The percentile of an observation is the fraction of observations with lower values. Using the above example, it is the fraction of people who scored less than Ann. If we graphed a curve of all test scores, it would be equal to the area under the curve, up to Ann\u2019s score on the x-axis. This is generally calculated on a computer or graphing calculator.</p>"},{"location":"Notes/Statistics/4.2%20Geometric%20distribution/","title":"4.2 Geometric distribution","text":"<p>Questions such as</p> <ul> <li>How long should we expect to flip a coin before it turns up heads?</li> <li>How many times should we expect to roll a die until we get a 1?</li> </ul> <p>can be answered using the geometric distribution.</p>"},{"location":"Notes/Statistics/4.2%20Geometric%20distribution/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>Insurance plans often have a deductible, where the insured individual is responsible for costs up to the deductible, and then costs above the deductible are shared between the individual and insurance company.</p> <p>Suppose an insurance company found that 70% of people they insure stay below their deductible in any given year. These people can be thought of as a trial. If a person does not exceed the deductible, we label them a success. If they do exceed the deductible, we label them a failure. Since 70% of individuals stay below their deductible, we denote the probability of a success as \\(p=0.7\\).</p> <p>When an individual trial only has two possible outcomes (often labeled success and failure) it is called a Bernoulli random variable. Usually a 1 is used for success and a 0 for failure, which makes things convenient:</p> <ul> <li>Suppose we have ten trials: 1, 1, 1, 0, 1, 0, 0, 1, 1, 0</li> <li>The sample proportion is the sample mean of these observations: \\(p = \\frac{successes}{trials}=(1+1+1+1+1+1) \\div 10 = 0.6\\)</li> </ul> <p>\ud83d\udca1 Bernoulli Random Variable: If \\(X\\) is a random variable that takes value 1 with a probability of success \\(p\\) and value 0 with probability \\(1-p\\), then \\(X\\) is a Bernoulli random variable with mean and standard deviation of \\(\\mu = p\\) \\(\\sigma=\\sqrt{p(1-p)}\\)</p>"},{"location":"Notes/Statistics/4.2%20Geometric%20distribution/#geometric-distribution","title":"Geometric Distribution","text":"<p>Geometric distribution is used to describe how many trials it takes to observe a success.</p> <p>What are the chances that the first person will not exceed their deductible (i.e. be a success)? The second? Third?</p> <p>The probability of stopping after the first person is the chance that this person will not hit their deductible - 0.7.</p> <p>There is a 30% chance that the first person will be a failure. So to calculate the second person, we do \\((0.3)(0.7)=0.21\\)</p> <p>The third case would be \\((0.3)(0.3)(0.7)\\)</p> <p>If the first success is on the \\(n^{th}\\) person, then there are \\(n-1\\) failures and 1 success, which corresponds to the probability \\((0.3)^{n-1}(0.7)\\), which is the same as \\((1-0.7)^{n-1}(0.7)\\)  \ud83d\udca1 Geometric Distribution If the probability of a success in one trial is \\(p\\) and the probability of a failure is \\(1-p\\), then the probability of finding the first success in the \\(n^{th}\\) trial is given by \\(\\large (1-p)^{n-1}p\\)</p> <p>The mean (i.e. expected value), variance, and standard deviation of this wait time are given by \\(\\large \\mu=1/p\\) \\(\\large \\sigma^2=(1-p)/p^2\\) \\(\\sigma=\\sqrt{\\frac{1-p}{p^2}}\\)</p>"},{"location":"Notes/Statistics/4.3%20Binomial%20distribution/","title":"4.3 Binomial distribution","text":"<p>Suppose the insurance agency from chapter 4.2 is conducting a random sample of four individuals. What is the chance that exactly one of them will exceed the deductible?</p> <p>$$ \\begin{align} P (A&amp;=\\text{ exceed, }B=\\text{ not, }C= \\text{ not, }D=\\text{not}) \\ &amp;=P(A=\\text{ exceed})P(B=\\text{ not})P(C=\\text{not})P(D=\\text{not}) \\ &amp;= (0.3)(0.7)(0.7)(0.7)\\ &amp;= (0.7)<sup>3(0.3)</sup>1\\ &amp;= 0.103</p> <p>\\end{align} $$</p> <p>But there are three other scenarios: B, C, or D could exceed while the rest don\u2019t. So the total probability is actually \\(4 \\times (0.7)^3(0.3)^1=0.412\\)</p> <p>Scenarios like this are examples of binomial distribution.</p> <p>The general formula for the number of ways to choose \\(k\\) successes in \\(n\\) trials (i.e. arrange \\(k\\) successes and \\(n-k\\) failures) is</p> \\[ \\large \\binom{n}{k}=\\frac{n!}{k!(n-k)!} \\] <p>\\(\\binom{n}{k}\\) is read as \\(n\\) choose \\(k\\).</p> <p>Example:</p> \\[ \\binom{4}{3}=\\frac{4!}{3!(4-3)!}=\\frac{4!}{3!1!}=\\frac{(4)(3)(2)(1)}{((3)(2)(1))(1)}=\\frac{28}{6}=4 \\] <p>If the example allows repetition, we use this formula instead:</p> \\[ C(n+k-1, k)=\\frac{(n+k-1)!}{k!(n-1)!} \\] <p>\ud83d\udca1 Binomial Distribution Suppose the probability of a single trial being a success is \\(p\\). Then the probability of observing exactly \\(k\\) successes in \\(n\\) independent trials is given by \\(\\large \\binom{n}{k}p^k(1-p)^{n-k}=\\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\) The mean is \\(\\mu = np\\) The variance is \\(\\sigma^2=np(1-p)\\), and the standard deviation is the square root of this.</p> <p>Is it binomial?</p> <ul> <li>Are trials independent?</li> <li>Is the number of trials (n) fixed?</li> <li>Can each trial outcome be classified as a success or failure?</li> <li>Is the probability of success (p) the same for each trial?</li> </ul>"},{"location":"Notes/Statistics/4.3%20Binomial%20distribution/#normal-approximation-to-the-binomial-distribution","title":"Normal Approximation to the Binomial Distribution","text":"<p>The binomial formula is cumbersome when the sample size is large. In some cases we can use the normal distribution as an easier and faster way to estimate binomial probabilities.</p> <p>\ud83d\udca1 Normal Approximation of the Binomial Distribution The binomial distribution with a probability of success \\(p\\) is nearly normal when the sample size \\(n\\) is sufficiently large that \\(np\\) and \\(n(1-p)\\) are both at least 10. The approximate normal distribution has parameters corresponding to the mean and standard deviation of the binomial distribution: \\(\\mu=np\\) \\(\\sigma=\\sqrt{np(1-p)}\\)</p>"},{"location":"Notes/Statistics/4.4%20Negative%20binomial%20distribution/","title":"4.4 Negative binomial distribution","text":"<p>Geometric distributions describe the probability of observing the first success on the \\(n^{th}\\) trial. The **********negative binomial distribution************ is more general, describing the probability of observing the \\(k^{th}\\) success on the \\(n^{th}\\) trial.</p> <p>\ud83d\udca1 Is it negative binomial? Checklist - Trials are independent -   Each trial outcome can be classified as success or failure -   The probability of success is the same for each trial -   The last trial *must* be a success</p>"},{"location":"Notes/Statistics/4.4%20Negative%20binomial%20distribution/#example","title":"Example","text":"<p>The probability of a success is 0.8. What is the probability that it takes 6 trials to get 4 successes?  These are the 10 ways that the 4th success could be achieved on the 6th trial. All of these have 2 failures and 4 successes, so the probability is the same: \\(0.2^2*0.8^4=0.016384\\). Thus, the probability that the 6th trial contains the 4th success is \\(10 * 0.016384 = 0.16384\\)</p> <p>We broke the problem into two parts:</p> <ol> <li>Find the probability of a single sequence</li> <li>Multiply it by the number of possible sequences</li> </ol> <p>How can we easily find the number of possible sequences?</p> \\[ \\binom{n-1}{k-1}=\\frac{(n-1)!}{(k-1)!((n-1)-(k-1))!}=\\frac{(n-1)!}{(k-1)!(n-k)!} \\] <p>This is the number of ways we can order \\(k - 1\\) successes and \\(n - k\\) failures in \\(n - 1\\) trials.</p> <p>\ud83d\udca1 Negative Binomial Distribution  The negative binomial distribution describes the probability of observing the \\(k^{th}\\) success on the \\(n^{th}\\) trial, where all trials are independent: \\(\\large P(k^{th}\\text{ success on }n^{th} \\text{ trial}) = \\binom{n-1}{k-1}p^k(1-p)^{n-k}\\) where \\(p\\) is the probability that an individual trial is a success.</p>"},{"location":"Notes/Statistics/4.5%20Poisson%20distribution/","title":"4.5 Poisson distribution","text":"<p>Poisson distribution is often useful for estimating the number events in a large population over a unit of time. </p> <p>The *rate* for a Poisson distribution is the average number of occurrences in a mostly-fixed population per unit of time. The parameter is the rate (i.e. how many events we expect to observe) and it is typically denoted by \\(\\lambda\\) or \\(\\mu\\). Using the rate, we can describe the probability of observing \\(k\\) events in a single unit of time.</p> <p>[!def] Poisson Distribution Suppose we are watching for events and the number of observed events follows a Poisson distribution with rate \\(\\lambda\\).  Then \\(\\large P(\\text{observe }k \\text{ events})=\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) The mean and standard deviation of this distribution are \\(\\lambda\\) and \\(\\sqrt{\\lambda}\\)</p> <p>A random variable may follow a Poisson distribution if we are looking for the number of events, the population that generates such events is large, and the events occur independently of each other.</p> <p>Some events that are not really independent can still follow a Poisson model - i.e. weekends are more popular for weddings. The idea of modeling rates for a Poisson distribution against a second variable such as the day of the week forms the foundation of some methods of generalized linear models.</p>"},{"location":"Notes/Statistics/5.1%20Point%20estimates%20and%20sampling%20variability/","title":"Point estimates and error","text":"<p>Suppose a poll suggested the US President's approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest.</p> <p>When the parameter is a proportion, it is often denoted by \\(p\\), and we often refer to the sample proportion as \\(\\hat{p}\\) (p-hat). Unless we collect responses from every person in the population, \\(p\\)  is unknown and we use \\(\\hat{p}\\) as the estimate of \\(p\\). The difference in the poll vs. the parameter is called the error in the estimate. </p> <p>Generally, the error consists of two aspects:</p> <ul> <li>Sampling error (aka sampling uncertainty: how much an estimate will tend to vary from one sample to the next<ul> <li>e.g. estimate from one sample might be 1% too low while in another it may be 3% too high</li> <li>sample size is often represented by \\(n\\)</li> </ul> </li> <li>Bias: a systematic tendency to over/underestimate the true population value.<ul> <li>e.g. wording questions in certain ways can provoke certain responses that don\u2019t indicate the true value</li> </ul> </li> </ul>"},{"location":"Notes/Statistics/5.1%20Point%20estimates%20and%20sampling%20variability/#understanding-the-variability-of-a-point-estimate","title":"Understanding the variability of a point estimate","text":"<p>Suppose the proportion of a population who supports a topic is \\(p = 0.88\\), which is the parameter of interest. If we took a poll of X number of people from the population, how close would the sample proportion be to the parameter of interest? In other words, how does the sample proportion \\(\\hat{p}\\) behave when the true population proportion is 0.88?</p> <p>We can simulate situations like this with computer code. Multiple simulations need to be run to get an accurate estimate. The distribution of sample proportions is called a sampling distribution. Sampling distributions have three areas of interest: the center (same as the parameter), the spread (standard deviation - aka standard error), and the shape.</p> <p></p>"},{"location":"Notes/Statistics/5.1%20Point%20estimates%20and%20sampling%20variability/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>The distribution above looks very similar to a normal distribution. This is due to the principle known as the Central Limit Theorem.</p> <p>[!def] Central Limit Theorem When observations are independent and the sample size is sufficiently large, the sample proportion \\(\\hat{p}\\) will tend to follow a normal distribution with the following mean \\(\\mu_{\\hat{p}}=p\\) and standard error \\(SE_{\\hat{p}}=\\sqrt{\\frac{p(1-p)}{n}}\\) If we do not know \\(p\\) we can use \\(\\hat{p}\\), which is known as a substitution approximation.</p> <p>The sample size is typically considered sufficiently large when \\(np \\ge 10\\) and \\(n(1-p)\\ge10\\), which is called the success-failure condition</p> <p>Sometimes another condition added is that samples from a population must not exceed 10% of the population. When the sample exceeds 10% of the population size, some simpler methods (that we use in this book) tend to overestimate the sampling error. v</p>"},{"location":"Notes/Statistics/5.1%20Point%20estimates%20and%20sampling%20variability/#verifying-that-sample-observations-are-independent","title":"Verifying that sample observations are independent","text":"<p>Subjects in an experiment are considered independent if they undergo random assignment to treatment groups.</p> <p>If observations are from a simple random sample, they are independent.</p> <p>If a sample is from a seemingly random process, e.g. an occasional error on an assembly line, checking independence is more difficult.</p>"},{"location":"Notes/Statistics/5.2%20Confidence%20intervals%20for%20a%20proportion/","title":"5.2 Confidence intervals for a proportion","text":"<p>\\(\\hat{p}\\) inherently has some standard error, so when stating an estimate for a population proportion, it is best practice to provide a plausible range of values instead of just the point estimate. </p> <p>Using only a point estimate is like fishing with a spear - we can throw a spear where we saw a fish, but we will probability miss. A confidence interval, however, is like fishing with a net; it provides a range of plausible values where we are likely to find the population parameter. </p> <p>The sample proportion \\(\\hat{p}\\) is the most plausible value of the population proportion. the standard error provides a guide for how large we should make the confidence interval. </p> <p>When the Central Limit Theorem conditions are satisfied, the point estimate closely follows a normal distribution. In a normal distribution, 95% of data points are within 1.96 standard deviations of the mean. Using this principle, we can construct a confidence interval that extends 1.96 standard errors from the sample proportion to be 95% confident that the interval captures the population proportion. </p> <p>point estimate \\(\\pm 1.96 \\cdot SE\\) \\(\\hat{p} \\pm 1.96 \\cdot \\sqrt{\\frac{p(1-p)}{n}}\\)</p> <p>\"95% confident\" means that if we took many samples and built a 95% confidence interval from each, about 95% of those intervals would contain the parameter, \\(p\\). </p> <p>[!def] 95% Confidence Interval for a Parameter When the distribution of a point estimate qualifies for the Central Limit Theorem and therefore closely follows a normal distribution, we can construct a 95% confidence interval as  \\(\\text{point estimate }\\pm 1.95 \\cdot SE\\) (SE = standard error)</p> <p>[!example] 5.8. A Pew research poll found that 88.7% of a random sample of 1000 Americans supported expanding the role of solar power. Compute and interpret a 95% confidence interval for he population proportion. The standard error is 0.010. \\(\\(\\begin{align} &amp;\\hat{p} \\pm 1.96 \\cdot SE_{\\hat{p}} \\\\ &amp;0.887 \\pm 1.96 \\cdot 0.010 \\\\ &amp; (0.8674, 0.9066) \\end{align}\\)\\) We are 95% confident that the actual proportion of Americans who support expanding solar power is betwen 86.7% and 90.7%</p>"},{"location":"Notes/Statistics/5.2%20Confidence%20intervals%20for%20a%20proportion/#changing-the-confidence-interval","title":"Changing the Confidence Interval","text":"<p>If \\(X\\) is a normally distributed random variable, what is the probability of the value \\(X\\) being within 2.58 standad deviations of the mean?</p> <p>This is equivalent to asking how often the Z-score \\(z\\) will be \\(-2.58 &lt; z &lt; 2.58\\). We can use statistical software, a calculator, or a table to look up these values for a normal distribution: 0.0049 and 0.9951. Thus, there is a \\(0.9951 - 0.0049 \\approx 0.99\\) probability that an unobserved normal random variable \\(X\\) will be within 2.58 standard deviations of \\(\\mu\\). </p> <p>This means that 99% of the time, a normal random variable will be within 2.58 standard deviations of the mean. to create a 99% confidence interva, we change 1.96 in the 95% confidence interval formula to 2.58. Therefore:</p> <p>[!formula] Formula for 99% confidence interval \\(\\text{point estimate } \\pm 2.58 \\cdot SE\\)</p> <p>What about when our model does not fit the normal distribution?</p> <p>[!def] Confidence interval using any confidence level If a point estimate closely follows a normal model with standard error SE, then a confidence interval for the population parameter is  \\(\\text{point estimate } \\pm z^* \\cdot SE\\) where \\(z^*\\) corresponds to the confidence level selected. </p> <p>[!def] Margin of error In a confidence interval, \\(z^* \\cdot SE\\) is called the margin of error.</p> <p>]]</p> <p>[!checklist] Confidence interval for a single proportion 1. Prepare - Identify \\(\\hat{p}\\) and \\(n\\) and determine the confidence level you want to use. 2. Check - Verify the conditions to ensure \\(\\hat{p}\\) is normal. For one-proportion confidence intervals, use \\(\\hat{p}\\) in place of \\(p\\) to check the success-failure condition.  3. Calculate - If the conditions hold, compute \\(SE\\) using \\(\\hat{p}\\), find \\(z^*\\), and construct the interval. 4. Conclude - Interpret the confidence interval in the context of the problem</p>"},{"location":"Notes/Statistics/5.2%20Confidence%20intervals%20for%20a%20proportion/#interpreting-confidence-intervals","title":"Interpreting Confidence Intervals","text":"<p>Confidence intervals are only about the population parameter. It says nothing about individual observations or point estimates. </p>"},{"location":"Notes/Statistics/5.3%20Hypothesis%20testing%20for%20a%20proportion/","title":"5.3 Hypothesis testing for a proportion","text":"<p>[!def] Null and Alternative Hypotheses The null hypothesis (\\(H_0\\)) often represents a skeptical perspective or a claim to be tested. The alternative hypothesis (\\(H_A\\)) represents an alternative claim under consideration and is often represented by a range of possible parameter values.</p> <p>Just because we don't find supporting evidence for the null hypothesis doesn't mean that the alternative is true. Think about a court case: even if the jurors leave unconvinced of guilt beyond a reasonable doubt, this does not mean they believe the defendant is innocent. </p>"},{"location":"Notes/Statistics/5.3%20Hypothesis%20testing%20for%20a%20proportion/#example","title":"Example","text":"<p>Question: How many of the world's 1 year old children have been vaccinated against some disease: a. 20% b. 50% c. 80%</p> <p>We ask this question to randomly selected people with a college degree and would like to determine if their guesses are as accurate as randomly guessing. That is, the proportion of people who pick the correct answer, is about 33.3%. This is our null hypothesis: \\(H_0: p = 0.333\\) \\(H_A: p \\ne 0.333\\)</p> <p>We want to make a conclusion about the population parameter \\(p\\). The value we are comparing the parameter to is called the null value - in this case, 0.333.</p> <p>We have sample data for 50 sampled adults. Of this group, 24% got the question correct. We want to know if this deviation from 33% is simply due to random chance, or if the data provides strong evidence that the population proportion is different from 33%. </p> <p>First, we need to check whether it is reasonable to construct a confidence interval for \\(p\\) using the sample data. If so, we construct a 95% confidence interval. </p> <ul> <li>Success-failure condition is passed<ul> <li>\\(\\hat{p} = 0.24\\)</li> <li>\\(n\\hat{p} = 12\\), \\(n(1-\\hat{p})=38\\)</li> <li>Trials are independent</li> </ul> </li> <li>To construct the confidence interval, we need the point estimate (0.24), the critical value for the 95% confidence interval (\\(z^* = 1.96\\)), and the standard error of \\(\\hat{p}\\) which in this case is \\(0.060\\). We can then construct the confidence interval:  We are 95% confident that the proportion of all college-educated adults to answer this question correctly is between 12.2% and 35.8%</li> </ul> <p>Since the null value (0.333) falls within the range of plausible value from the confidence interval, we cannot say the null value is implausible.</p> <p>[!info] Compare the p-value to \\(\\alpha\\) to evaluate \\(H_0\\) When the p-value is less than the significance level, \\(\\alpha\\), reject \\(H_0\\), and report that the data provide strong evidence supporting the alternative hypothesis. When the p-value is greater than \\(\\alpha\\), do not reject \\(H_0\\), and report that we do not have sufficient evidence to reject the null hypothesis. </p>"},{"location":"Notes/Statistics/6.1%20Inference%20for%20a%20single%20proportion/","title":"6.1 Inference for a single proportion","text":"<p>[!def] Sampling distribution of \\(\\hat{p}\\) The sampling distribution for \\(\\hat{p}\\) based on a sample size \\(n\\) from a population with a true proportion \\(p\\) is nearly normal when: 1. The sample's observations are independent, e.g. are from a simple random sample 2. We expected to see at least 10 successes and 10 failures in the sample, i.e. \\(np \\ge 10\\) and \\(n(1-p)\\ge 10\\) (success-failure condition) When these conditions are met, the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(p\\) and standard error \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\)</p> <p>A confidence interval provides a range of plausible values for the parameter \\(p\\), and when \\(\\hat{p}\\) can be modeled using a normal distribution, the confidence interval for \\(p\\) takes the form \\(\\hat{p} \\pm z^* \\times SE\\)</p> <p>[!check] Confidence interval for a single proportion 1. Prepare: Identify \\(\\hat{p}\\) and \\(n\\), and determine what confidence level you wish to use. 2. Check: Verify the conditions to ensure \\(\\hat{p}\\) is normal. Use \\(\\hat{p}\\) in place of \\(p\\) to check the success-failure condition. 3. Calculate: If the conditions hold, compute \\(SE\\) using \\(\\hat{p}\\), find \\(z^*\\), and construct the interval. 4. Conclude: Interpret the confidence interval in the context of the problem.</p> <p>[!check] Hypothesis testing for a single proportion 1. Prepare: Identify the parameter of interest, list hypotheses, identify the significance level, and identify \\(\\hat{p}\\) and \\(n\\) 2. Check: Verify conditions to ensure \\(\\hat{p}\\) is nearly normal under \\(H_0\\). Use the null value (\\(p_0\\)) to check the success-failure condition. 3. Calculate: If the conditions hold, compute the standard error using \\(p_0\\)  4. Conclude: Evaluate the hypothesis test by comparing the p-value to \\(\\alpha\\), and provide a conclusion in the context of the problem. </p>"},{"location":"Notes/Statistics/6.1%20Inference%20for%20a%20single%20proportion/#when-conditions-arent-met","title":"When conditions aren't met","text":"<p>What happens when the success-failure condition fails? Or the independence condition fails? The strategy to generate the interval or p-value change.  When the success-failure condition isn't met, we can simulate the null distribution of \\(\\hat{p}\\) using the null value, \\(p_0\\). For a confidence interval when the success-failure condition isn't met, the Clopper-Pearson interval is used.</p>"},{"location":"Notes/Statistics/6.1%20Inference%20for%20a%20single%20proportion/#choosing-a-sample-size-when-estimating-a-proportion","title":"Choosing a sample size when estimating a proportion","text":"<p>Often a sample size is chosen to be large enough that the margin of error is sufficiently small that the sample is useful. </p> <p>The margin of error for a sample proportion is \\(z^* \\sqrt{\\frac{p(1-p)}{n}}\\).</p> <p>[!example] A university is doing a survey to determine what fraction of students support a $200 per year tuition increase to pay for a new building. How big of a sample is required to ensure the margin of error is smaller than 0.04 using a 95% confidence interval? Our goal is to find the smallest value of \\(n\\) such that the margin of error is smaller than 0.04. A 95% confidence interval corresponds to \\(z^* = 1.96\\) \\(1.96 \\times \\sqrt{\\frac{p(1-p)}{n}} \\lt 0.04\\)</p> <p>There are two unknowns: \\(p\\) and \\(n\\). If we have an estimate of \\(p\\), we can use that to solve for \\(n\\). If we don't have such an estimate, we use the worst case value 0.5 for \\(p\\). \\(1.96 \\times \\sqrt{\\frac{0.5(1-0.5)}{n}} \\lt 0.04\\) \\(1.96^2 \\times \\frac{0.25}{n} \\lt 0.04^2\\) \\(1.96^2 \\times \\frac{0.25}{0.04^2} \\lt n\\) \\(600.25 \\lt n\\) We would need over 600.25 (so 601) participants to ensure the sample proportion is whithin 0.04 of the true proportion within 95% confidence.</p>"},{"location":"Notes/Statistics/6.2%20Difference%20of%20two%20proportions/","title":"6.2 Difference of two proportions","text":"<p>This chapter extends the methods of the previous chapter to apply confidence intervals and hypothesis tests to differences in population proportions: \\(p_1 - p_2\\). </p>"},{"location":"Notes/Statistics/6.2%20Difference%20of%20two%20proportions/#sampling-distribution-of-the-difference-of-two-proportions","title":"Sampling distribution of the difference of two proportions","text":"<p>[!def] Conditions for the sampling distribution of the difference of two proportions to be normal \\(\\hat{p_1} - \\hat{p_2}\\) can be modeled using a normal distribution when: - Independence, extended: The data are independent within and between the two groups. Generally satisfied if the data come from two independent random samples or if the data come from a randomized experiment.  - Success-failure condition: The success-failure condition holds for both groups (checked separately) When these conditions are satisfied, the standard error of \\(\\hat{p_1} - \\hat{p_2}\\) is \\(SE = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\) where \\(p_1\\) and \\(p_2\\) represent the population proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes.</p> <p>We can apply the generic confidence interval formula for a difference of two proportions, using \\(\\hat{p_1} - \\hat{p_2}\\) as the point estimate and substituting the SE formula: \\(\\(\\hat{p_1} - \\hat{p_2} \\pm z^* \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\)\\)</p>"},{"location":"Notes/Statistics/6.2%20Difference%20of%20two%20proportions/#hypothesis-testing-for-a-difference-of-proportions","title":"Hypothesis testing for a difference of proportions","text":"<p>When the null hypothesis is that the proportions are equal (i.e. this type of test is not more effective than the other), use the pooled proportion to verify the success-failure condition and estimate the standard error. \\(\\large \\hat{p}_{pooled}=\\frac{\\text{number of successes}}{\\text{number of cases}}=\\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1+n_2}\\)</p> <p>Are mammograms effective?</p> Died Lived Mammogram 500 44425 Control 505 44405 <p>The null hypothesis here (that mammograms are not effective) is \\(p_1 - p_2 = 0\\). We use a pooled proportion to check the success-failure condition: \\(\\(\\large \\hat{p}_{pooled} = \\frac{\\text{total \\# of patients who died}}{\\text{total \\# of patients in study}} = \\frac{500+505}{500+44425+505+44405}=0.0112\\)\\)</p> <p>This is an estimate of the breast cancer death rate across the entire study, and it's our best estimate of the proportions if the null hypothesis is true, that \\(p_1 = p_2\\).</p> <p>Is it reasonable to model the difference in proportions using a normal distribution?  Since all value are at least 10, the success-failure condition is satisfied. </p> <p> </p>"},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/","title":"6.3 Testing for goodness of fit using chi square method","text":"<p>The method described in this section is commonly used in two circumstances: - Given a sample of cases that can be classified into several groups, determine if the sample is representative of the general population - Evaluate whether data resemble a particular distribution, such as a normal distribution or a geometric distribution</p>"},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/#example","title":"Example","text":"<p>Are jurors racially representative of the overall population?  \\(H_0\\): The jurors are a random sample, i.e. there is no racial bias, and the observed counts reflect natural sampling fluctuation. \\(H_A\\): The jurors are not randomly sampled, i.e. there is racial bias in juror selection. </p> <p>In previous hypothesis tests, we constructed a test statistic of the following form: (point estimate - null value) / SE of point estimate. In this scenario, we must construct a test statistic for each group: - White     - \\(Z_1 = \\frac{205-198}{\\sqrt{198}} = 0.50\\) - Black     - \\(Z_2=\\frac{26-19.25}{\\sqrt{19.25}}=1.54\\) - Hispanic     - \\(Z_3=\\frac{25-33}{\\sqrt{33}}=-1.39\\) - Other     - \\(Z_4=\\frac{19-24.75}{\\sqrt{24.75}}=-1.16\\) We take the squared value of each of these Z's and sum them. Using the squared value has two effects: - Any negative values are now positive - Unusual differences will become much larger and easier to notice \\(Z^2_1 + Z^2_2 + Z^2_3 + Z^2_4 = 5.89 = X^2\\) \\(X^2\\) summarizes how strongly the observed counts tend to deviate from the null counts. If the null hypothesis is true, then \\(X^2\\) follows a chi-square distribution.</p> <p>The chi-square distribution is sometimes used to characterize data sets and statistics that are always positive and typically right skewed. </p> <p>While a normal distribution has two parameters - mean and standard deviation - chi-square only has one, called degrees of freedom (df).  As df gets larger,  - the center becomes larger - the variability increases - the distribution is more symmetric</p>"},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/#finding-a-p-value-for-a-chi-square-distribution","title":"Finding a p-value for a chi-square distribution","text":"<p>A large \\(X^2\\) value suggests strong evidence favouring the alternative hypothesis (that there was racial bias). However, we cannot quantify what the chance was of observing such a large test statistic if the null hypothesis actually was true. This is where the chi-square distribution comes in. If the null hypothesis is true and there is no racial bias, \\(X^2\\) would follow a chi-square distribution with three degrees of freedom. Under certain conditions, \\(X^2\\) follows a chi-square distribution with \\(k-1\\) degrees of freedom, where \\(k\\) is the number of bins. </p>"},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/#definitions","title":"Definitions","text":""},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/#chi-square-test-for-one-way-table","title":"Chi-square test for one-way table","text":"<p>Suppose we are to evaluate whether there is convincing evidence that a set of observed counts \\(O_1, O_2,...O_k\\) in \\(k\\) categories are unusually different from what might be expected under a null hypothesis. Call the expected counts that are based on the null hypothesis \\(E_1,E_2,...E_k\\). If each expected count is at least 5 and the null hypothesis is true, then the test statistic below follows a chi-square distribution with \\(k-1\\) degrees of freedom: \\(\\(X^2=\\frac{(O_1-E_1)^2}{E_1}+\\frac{(O_2-E_2)^2}{E_2}+...+\\frac{(O_k-E_k)^2}{E_k}+\\)\\) The p-value for this test statistic is found by looking at the upper tail of this chi-square distribution. We consider the upper tail because large values of \\(X^2\\) would provide greater evidence against the null hypothesis. </p>"},{"location":"Notes/Statistics/6.3%20Testing%20for%20goodness%20of%20fit%20using%20chi-square%20method/#conditions-for-chi-square-test","title":"Conditions for Chi-square test","text":"<p>There are two conditions that must be checked before performing a chi-square test: - Independence - Each case must be independent of all others in the table - Sample size/distribution - each scenario must have at least 5 expected cases</p>"},{"location":"Notes/Statistics/6.4%20Testing%20for%20independence%20in%20two-way%20tables/","title":"Difference between one-way and two-way tables","text":"<p>A one-way table describes counts for each outcome in a single variable. A two-way table describes counts for combinations of outcomes for two variables.  when we consider a two-way table, we often would like to know if the variables are related in any way; that is, are they dependent or independent?</p>"},{"location":"Notes/Statistics/6.4%20Testing%20for%20independence%20in%20two-way%20tables/#computing-expected-counts-in-a-two-way-table","title":"Computing expected counts in a two-way table","text":"<p>To identify the expected count for the \\(i^{th}\\) row and \\(j^{th}\\) column, compute \\(\\(\\text{Expected Count}_{\\text{row }i, \\text{col }j}=\\frac{(\\text{row } i \\text{ total}) \\times (\\text{column } j \\text{ total})}{\\text{table total}}\\)\\)</p>"},{"location":"Notes/Statistics/6.4%20Testing%20for%20independence%20in%20two-way%20tables/#chi-square-test-for-two-way-tables","title":"Chi-square test for two-way tables","text":"<p>Computed the same way as a one-way table. Compute (observed count - expected count)\\(^2\\) / expected count for each cell and take the sum to find \\(X^2\\).</p>"},{"location":"Notes/Statistics/6.4%20Testing%20for%20independence%20in%20two-way%20tables/#degrees-of-freedom-for-two-way-tables","title":"Degrees of freedom for two-way tables","text":"<p>\\(df =\\) (number of rows minus 1) \\(\\times\\) (number of columns minus 1)</p>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/","title":"7.1 One sample means with the t distribution","text":"<p>The sample mean \\(\\bar{x}\\) (the mean value of all samples) can be modeled using a normal distribution when certain conditions are met. </p> <p>[!def] Central Limit Theorem for the sample mean When we collect a sufficiently large sample of \\(n\\) independent observations from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of \\(\\bar{x}\\) will be nearly normal with a mean of \\(\\mu\\) and a Standard Error (SE) of \\(\\sigma \\div \\sqrt{n}\\)</p> <p>However, when the standard deviation is not known, we have to use the \\(t\\)-distribution. </p>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#the-two-conditions-required-for-modeling-barx","title":"The two conditions required for modeling \\(\\bar{x}\\)","text":"<ol> <li>Independence: the sample observations must be independent, i.e. simple random sample, or a sample of a random process like rolling a die</li> <li>Normality: When a sample is small, the observations must come from a normally distributed population.</li> </ol>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#how-to-perform-the-normality-check","title":"How to perform the normality check","text":"<p>There is no perfect way to check if a sample is normal, so instead two rules of thumb are used:  - \\(n &lt; 30\\): If sample size \\(n\\) is less than 30 and there are no clear outliers in the data, we assume the data come from a nearly normal distribution - \\(n \\ge 30\\): If the sample size \\(n\\) is at least 30 and there are no particularly extreme outliers, then we assume the sampling distribution of \\(\\bar{x}\\) is nearly normal, even if the distribution of individual observations is not. </p>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#the-t-distribution","title":"The \\(t\\)-distribution","text":"<p>We cannot directly calculate the standard error for \\(\\bar{x}\\) since we rarely know the population standard deviation \\(\\sigma\\) in the real world.  We can estimate the standard error similar to how we estimate the standard error for a sample proportion: \\(\\(SE=\\frac{\\sigma}{\\sqrt{n}}\\approx\\frac{s}{\\sqrt{n}}\\)\\) However, when the sample size is small, this leads to problems when using the normal distribution to model \\(\\bar{x}\\). Instead, we use the \\(t\\)-distribution, which looks similar to a normal distribution, but with thicker tails. It has one parameter - degrees of freedom, \\(df\\) - and as \\(df\\) increases, the \\(t\\)-distribution approaches a normal distribution.</p> <p></p> <p>[!def] Degrees of freedom (\\(df\\)) The degrees of freedom describes the shape of the \\(t\\)-distribution. The larger the degrees of freedom, the more closely the distribution approximates a normal model. When modeling \\(\\bar{x}\\) using the \\(t\\)-distribution, use \\(df=n-1\\).</p> <p>Similar to the normal distribution, we use either statistical software or a table to find the values of the tails. </p>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#one-sample-t-confidence-intervals","title":"One sample \\(t\\)-confidence intervals","text":"<p>In the normal model, we use \\(z^*\\) and the standard error to determine the width of a confidence interval. The formula is similar with \\(t\\)-distributions: \\(\\(\\bar{x}\\pm t^*_{df}\\times\\frac{s}{\\sqrt{n}}\\)\\) Similar to normal distribution, we use a table or software to find the cutoff point corresponding to the confidence level, \\(t^*_{df}\\)</p>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#confidence-interval-for-a-single-mean","title":"Confidence interval for a single mean","text":"<ol> <li>Prepare: Identify \\(\\bar{x}, s, n\\) and determine what confidence level you wish to use</li> <li>Check: Verify conditions to ensure \\(\\bar{x}\\) is nearly normal</li> <li>Calculate: If conditions hold, compute \\(SE\\), find \\(t^*_{df}\\), and construct the interval</li> <li>Conclude: Interpret the confidence interval in the context of the problem</li> </ol>"},{"location":"Notes/Statistics/7.1%20One-sample%20means%20with%20the%20t-distribution/#hypothesis-testing-for-a-single-mean","title":"Hypothesis testing for a single mean","text":"<ul> <li>Prepare: Identify parameter of interest, list out hypotheses, identify significance level, identify \\(\\bar{x}, s, n\\)</li> <li>Check: Verify conditions to ensure \\(\\bar{x}\\) is nearly normal</li> <li>Calculate: If conditions hold, compute \\(SE\\), compute T-score, identify p-value</li> <li>Conclude: Evaluate the hypothesis test by comparing the p-value to the significance level \\(\\alpha\\), and provide a conclusion in the context of the problem.</li> </ul>"},{"location":"Notes/Statistics/7.2%20Paired%20data/","title":"Paired Data","text":"<p>Two sets of observations are paired if each observation in one set has a special correspondence or connection with exactly one observation in the other data set.</p> <p>To analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations. </p>"},{"location":"Notes/Statistics/7.2%20Paired%20data/#inference-for-paired-data","title":"Inference for Paired Data","text":"<p>To analyze a paired data set, simply analyze the differences and use the same \\(t\\)-distribution techniques applied in [[7.1 One-sample means with the t-distribution|7.1]]. </p>"},{"location":"Notes/Statistics/7.3%20Difference%20of%20two%20means/","title":"7.3 Difference of two means","text":"<p>The point estimate of a difference is found by taking the difference in the sample means:  \\(\\(\\large \\bar{x}_{esc} - \\bar{x}_{control}\\)\\)</p>"},{"location":"Notes/Statistics/7.3%20Difference%20of%20two%20means/#using-the-t-distribution-for-a-difference-in-means","title":"Using the \\(t\\)-distribution for a difference in means","text":"<p>The \\(t\\)-distribution can be used for inference when working with the standardized difference of two means if - Independence, extended: The data ase independent within and between the two groups, e.g. the data come from independent random samples or from a randomized experiment - Normality: We check the outliers rules of thumb for each group separately.  The standard error may be computed as \\(\\(\\large SE=\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\)\\) If statistical software isn't available, use the smaller of \\(n_1-1\\) and \\(n_2-1\\) for the degrees of freedom. </p> <p>As with one-sample cases, compute the standard error using the sample standard deviations rather than the population standard deviations. </p>"},{"location":"Notes/Statistics/7.4%20Power%20calculations%20for%20a%20difference%20of%20means/","title":"7.4 Power calculations for a difference of means","text":"<p>Often in experiment planning, we want to collect as much data as possible, but collecting data can be expensive, and there may be risk to patients depending on the experiment.</p>"},{"location":"Notes/Statistics/7.4%20Power%20calculations%20for%20a%20difference%20of%20means/#computing-the-power-for-a-2-sample-test","title":"Computing the power for a 2-sample test","text":"<p>When planning a study, we want to know how likely we are to detect an effect we care about. In other words, if there is a real effect, and that effect is large enough that it has practical value, then what is the probability that we detect that effect? This probability is called the power and we can compute it for different sample sizes or for different effect sizes.</p> <p>We first determine what is a practically significant result. Suppose that company researchers care about finding any effect on blood pressure that is 3 mmHg or larger vs. the standard medication. Here, 3 mmHg is the minimum effect size and we want to know how likely we are to detect this size of an effect in the study. </p> <p>We need to determine an appropriate sample size to ensure we can be reasonably confident that we'll detect any effects that are practically important in a study. </p>"},{"location":"Notes/Statistics/7.5%20Comparing%20many%20means%20with%20ANOVA/","title":"7.5 Comparing many means with ANOVA","text":"<p>When we have more than two means to compare, we use a method called analysis of variance (ANOVA) and a new test statistic called \\(F\\). ANOVA uses a single hypothesis test to check whether the means across many groups are equal: - \\(H_0\\): The mean outcome is the same across all groups. - \\(H_A\\): At least one mean is different.</p> <p>Three conditions must be checked before performing ANOVA: - the observations are independent within and across groups - the data within each group are nearly normal - the variability across groups is about equal</p> <p>Strong evidence favouring the alternative hypothesis in ANOVA is described by unusually large differences among the group means. Assessing the variability of the group means relative to the variability among individual observations within each group is key to ANOVA's success.</p>"},{"location":"Notes/Statistics/7.5%20Comparing%20many%20means%20with%20ANOVA/#anova-and-the-f-test","title":"ANOVA and the \\(F\\)-test","text":"<p>ANOVA in this context focuses on one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? </p> <p>The variability between many groups is called the mean square between groups (MSG), and it has an associated degrees of freedom, \\(df_G = k-1\\), when there are \\(k\\) groups. The MSG can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn't be too large. </p> <p>The MSG alone isn't very useful - we need a benchmark to compare it to. We compute a pooled variance estimate, called the mean square error (MSE) which has an associated degrees of freedom value \\(df_E = n-k\\). MSE is essentially a measure of the variability within the groups. </p> <p>When the null hypothesis is true, any differences among the sample means are only due to chance, and the MSG and MSE should be about equal. The test statistic \\(F\\) is given by \\(\\(\\large F = \\frac{MSG}{MSE}\\)\\) We can use the \\(F\\) statistic to evaluate the hypotheses in what is called an \\(F\\)-test.</p> <p>A p-value can be computed from the \\(F\\) statistic using an \\(F\\)-distribution, which has two parameters: \\(df_1\\) and \\(df_2\\). For the \\(F\\) statistic in ANOVA, \\(df_1 = df_G\\) and \\(df_2 = df_E\\). </p> <p>The larger the observerd variability in the sample means (MSG) relative to the within-group observations (MSE), the larger \\(F\\) will be and the stronger the evidence against the null hypothesis. Since larger valuse of \\(F\\) represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a p-value.</p> <p>[!def] The \\(F\\) statistic and the \\(F\\)-test Analysis of variance (ANOVA) is used to test whether the mean outcome differs across 2 or more groups. ANOVA uses a test statistic \\(F\\), which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If \\(H_0\\) is true and the model conditions are satisfied, the statistic \\(F\\) follows an \\(F\\) distribution with parameters \\(df_1=k-1\\) and \\(df_2=n-k\\). The upper tail of the \\(F\\) distribution is used to represent the p-value.</p>"},{"location":"Notes/Statistics/7.5%20Comparing%20many%20means%20with%20ANOVA/#extras","title":"Extras","text":""},{"location":"Notes/Statistics/7.5%20Comparing%20many%20means%20with%20ANOVA/#formula-for-msg","title":"Formula for MSG","text":"<p>\\(\\(\\large MSG=\\frac{1}{df_G}SSG=\\frac{1}{k-1}\\sum_{i=1}^{k}n_i(\\bar{x}_i-\\bar{x})^2\\)\\) where \\(SSG\\) is the sum of squares between groups and \\(n_i\\) is the sample size of group \\(i\\). </p>"},{"location":"Notes/Statistics/8.1%20Fitting%20a%20line%2C%20residuals%2C%20and%20correlation/","title":"Fitting a line to data","text":"<p>Linear variables can be perfectly modeled with a straight line, but this is rarely the case in the real world. </p> <p>Linear regression is the statistical method for fitting a line to data where the relationship between two variables can be modeled by a straight line with some error: \\(\\(\\large y = \\beta_0 + \\beta_1 x + \\epsilon\\)\\) The \\(\\beta\\) values represent the model's parameters, and the error is represented by \\(\\epsilon\\). When we use x to predict y, we usually call x the explanatory or predictor variable and y the response variable. </p> <p>Residuals are the leftover variation in the data after accounting for the model fit. If an observation is above the regression line, its residual is positive, and if it is below, it is negative.</p> <p>[!def] Residual: The difference between observed and expected The residual of the \\(i^{th}\\) observation \\((x_i, y_i)\\) is the difference of the observed response and the response we would predict based on the model fit \\(\\(e_i=y_i-\\hat{y}_i\\)\\)</p>"},{"location":"Notes/Statistics/8.1%20Fitting%20a%20line%2C%20residuals%2C%20and%20correlation/#describing-linear-relationships-with-correlation","title":"Describing linear relationships with correlation","text":"<p>Correlation always takes values between -1 and 1, and describes the strength of the linear relationship between two variables. Correlation is denoted by \\(R\\). </p> <p>Formula for correlation (beyond scope of this course): \\(\\(R = \\frac{1}{n-1} \\sum_{i=1}^{n} \\frac{x_i-\\bar{x}}{s_x} \\frac{y_i - \\bar{y}}{s_y}\\)\\) where \\(\\bar{x}\\), \\(\\bar{y}\\) are the sample means and \\(s_x\\) and \\(s_y\\) are the standard deviations of each variable.</p> <p>The correlation is intended to quantify the strength of a linear trend. Nonlinear trends (even when strong) sometimes produce correlations that do not reflect the strength of the relationship.</p>"},{"location":"Notes/Statistics/8.2%20Least%20squares%20regression/","title":"An objective measure for finding the best line","text":"<p>When talking about the \"best line\" we are talking about the line with the smallest residuals. A common practice is to choose the line that minimizes the sum of the squared residuals: \\(\\(e_1^2+e_2^2+...+e_n^2\\)\\) This line is known as the least squares line. </p>"},{"location":"Notes/Statistics/8.2%20Least%20squares%20regression/#conditions-for-the-least-squares-line","title":"Conditions for the least squares line","text":"<p>When fitting a least squares line, we generally require - Linearity: The data should show a linear trend - Nearly normal residuals - Constant variability: The variability of points around the line remains roughly constant.  - Independent observations</p>"},{"location":"Notes/Statistics/8.2%20Least%20squares%20regression/#finding-the-least-squares-line","title":"Finding the least squares line","text":"<p>The slope of the least squares line can be estimated by \\(\\(\\large b_1=\\frac{s_y}{s_x}R\\)\\)</p> <p>The point \\((\\bar{x}, \\bar{y})\\) should be on the least squares line.</p> <p>The point-slope is given by \\(\\(\\large y - y_0 = b_1 \\times (x - x_0)\\)\\)  The slope describes the estimated difference in the \\(y\\) variable if the explanatory variable \\(x\\) for a case happened to be one unit larger. The intercept describes the average outcome of \\(y\\) if \\(x = 0\\) and the linear model is valid all the way to \\(x = 0\\) (which is often not the case). </p>"},{"location":"Notes/Statistics/8.2%20Least%20squares%20regression/#using-r2-to-describe-the-strength-of-a-fit","title":"Using \\(R^2\\) to describe the strength of a fit","text":"<p>It is more common to evaluate the strength of a linear relationship between two variables using \\(R^2\\) as opposed to just \\(R\\). </p>"}]}